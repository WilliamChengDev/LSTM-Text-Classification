{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a5a9d5",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f51930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "if not os.path.exists(\"data/aclImdb_v1.tar.gz\"):\n",
    "        #download database\n",
    "        print(\"downloading database...\")\n",
    "        urllib.request.urlretrieve(\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \"data/aclImdb_v1.tar.gz\")\n",
    "        print(\"download complete\")\n",
    "\n",
    "if not os.path.exists(\"data/aclImdb/\"):\n",
    "        #extract database\n",
    "        print(\"extracting database...\")\n",
    "        with tarfile.open(\"data/aclImdb_v1.tar.gz\", \"r:gz\") as tar:\n",
    "                tar.extractall(path=\"data\")\n",
    "        print(\"database extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa68431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "     # Loop over the two splits: training and testing\n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "\n",
    "        # Loop over both sentiment categories: positive and negative\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "\n",
    "            # Construct path to all text files of the current split and sentiment\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "\n",
    "            # Read each review text file\n",
    "            for f in files:\n",
    "                with open(f, encoding=\"utf-8\") as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Assign label 1 for 'pos' and 0 for 'neg'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "\n",
    "            # Sanity check: ensure that every text has a matching label\n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74031e33",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 803: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data, labels = \u001b[43mread_imdb_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIMDb reviews: train = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m pos / \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m neg, test = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m pos / \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m neg\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m      3\u001b[39m             \u001b[38;5;28mlen\u001b[39m(data[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m'\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(data[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mneg\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m      4\u001b[39m             \u001b[38;5;28mlen\u001b[39m(data[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m'\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(data[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mneg\u001b[39m\u001b[33m'\u001b[39m])))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mread_imdb_data\u001b[39m\u001b[34m(data_dir)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(f) \u001b[38;5;28;01mas\u001b[39;00m review:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         data[data_type][sentiment].append(\u001b[43mreview\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     26\u001b[39m         \u001b[38;5;66;03m# Assign label 1 for 'pos' and 0 for 'neg'\u001b[39;00m\n\u001b[32m     27\u001b[39m         labels[data_type][sentiment].append(\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sentiment == \u001b[33m'\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\encodings\\cp1252.py:23\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'charmap' codec can't decode byte 0x9d in position 803: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDb reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if not os.path.exists(\"data/data.json\"):\n",
    "        # Save the loaded review texts into a JSON file\n",
    "        json.dump(data, open('data/data.json', 'w'))\n",
    "        # Save the sentiment labels into another JSON file\n",
    "        json.dump(labels, open('data/labels.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb500d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and load the movie review data from 'data.json'\n",
    "f=open('data/data.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "# Open and load the sentiment labels from 'labels.json'\n",
    "f=open('data/labels.json')\n",
    "labels = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd8882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "print(\"IMDb reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27afbbb",
   "metadata": {},
   "source": [
    "### 1. Understanding the Data\n",
    "function that gets the average, maximum, and minimum word length of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understanding_data(data, set):\n",
    "        maxlength = 0\n",
    "        minlength = 99999999999999999999999\n",
    "        total = 0\n",
    "\n",
    "        for sentiment in data[set]:\n",
    "                for entry in data[set][sentiment]:\n",
    "                        word_count = len(entry.split())\n",
    "                        total += word_count\n",
    "                        if word_count > maxlength:\n",
    "                                maxlength = word_count\n",
    "                        if word_count < minlength:\n",
    "                                minlength = word_count\n",
    "        set_size = len(data[set]['pos'])+len(data[set]['neg'])\n",
    "        print(\"{} set avg word count: {}, max word count: {}, min word count {}\".format(set, total/set_size, maxlength, minlength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f110d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set avg word count: 233.7872, max word count: 2470, min word count 10\n",
      "test set avg word count: 228.52668, max word count: 2278, min word count 4\n"
     ]
    }
   ],
   "source": [
    "understanding_data(data, \"train\")\n",
    "understanding_data(data, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58448b0",
   "metadata": {},
   "source": [
    "#### Word Length Results:\n",
    "| | avg word count| max word count | min word count|\n",
    "|---|---|---|---|\n",
    "|train set| 233.7872 | 2470 | 10|\n",
    "|test set | 229.52688 | 2278 | 4|\n",
    "\n",
    "The word count of the reviews seem to vary quite drastically, with the longest review being almost 2500 words,\\\n",
    "and the shortest being only 4, and an average of about 230 words. Since the LSTM model reads one word at at time,\\\n",
    "reviews that are very long (~2500 words) might make training take significantly more time/memory, while on the\\\n",
    "other hand reviews that are very short (<10 words) probably won't give the model enough context to train on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341960e8",
   "metadata": {},
   "source": [
    "### Creating a Balanced Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def split_data(data, val_size = 3000):\n",
    "        shuffle(data['train']['pos'], labels['train']['pos'], random_state=420) #keeps the shuffling consistent\n",
    "        shuffle(data['train']['neg'], labels['train']['neg'], random_state=420)\n",
    "\n",
    "        val_size_pos = int(val_size/2)\n",
    "        val_size_neg = val_size - val_size_pos\n",
    "\n",
    "        train_set = data['train']['pos'][val_size_pos:] + data['train']['neg'][val_size_neg:]\n",
    "\n",
    "        val_set = data['train']['pos'][:val_size_pos] + data['train']['neg'][:val_size_neg]\n",
    "\n",
    "        test_set = data['test']['pos'] + data['test']['neg']\n",
    "\n",
    "        print(\"Train: {} | Val: {} | Test: {}\".format(\n",
    "                len(train_set), len(val_set), len(test_set)\n",
    "        ))\n",
    "        print(\"IMDB data: train = pos {} / neg {} , val = pos {} / neg {}, test = pos {} / neg {}\".format(\n",
    "                len(data['train']['pos']) - val_size_pos, len(data['train']['neg']) - val_size_neg, \n",
    "                val_size_pos, val_size_neg, \n",
    "                len(data['test']['pos']), len(data['test']['neg'])\n",
    "        ))\n",
    "\n",
    "        return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de884148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 22000 | Val: 3000 | Test: 25000\n",
      "IMDB data: train = pos 11000 / neg 11000 , val = pos 1500 / neg 1500, test = pos 12500 / neg 12500\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, test_set = split_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750eea24",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Convert text to lowercase\n",
    "- Remove punctuation\n",
    "- Tokenize text\n",
    "- Remove stop words\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line imports the Natural Language Toolkit (NLTK) library, which provides various tools and resources for working with human language data.\n",
    "import nltk\n",
    "\n",
    "# This imports the stopwords module from NLTK, which contains a list of common English words (like \"the\", \"a\", \"is\")\n",
    "# that often don't carry significant meaning in text analysis and are usually removed.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# This imports the PorterStemmer class from NLTK.\n",
    "# Stemming is the process of reducing words to their root or base form (e.g., \"running\" becomes \"run\").\n",
    "# The Porter stemmer is a widely used algorithm for this purpose.\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# This imports the re module, which provides support for regular expressions. Regular expressions are powerful tools for pattern matching and text manipulation.\n",
    "import re\n",
    "\n",
    "#  This imports the BeautifulSoup library, which is used for parsing HTML and XML documents. It's helpful for extracting text content from web pages or documents that might contain HTML tags.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# This defines a function named review_to_words that takes a single argument review, which is expected to be a string containing the text of a movie review.\n",
    "def review_to_words(review):\n",
    "    # This line downloads the list of stopwords from NLTK if it hasn't been downloaded already. The quiet=True argument suppresses the download output.\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "    # This creates an instance of the PorterStemmer class, which we'll use later for stemming words.\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # This line uses BeautifulSoup to parse the input review as an HTML document (\"html.parser\" specifies the parser to use).\n",
    "    # Then, .get_text() extracts the visible text content, effectively removing any HTML tags that might be present in the review.\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "\n",
    "    # text.lower(): It converts the entire text to lowercase. This ensures that words like \"The\" and \"the\" are treated as the same.\n",
    "    # re.sub(r\"[^a-zA-Z0-9]\", \" \", ...): It replaces any character that is not an uppercase letter (a-z), a lowercase letter (A-Z), or a digit (0-9) with a space.\n",
    "    # This helps in removing punctuation marks and other special characters.\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    # This line splits the processed text into a list of individual words using whitespace as the delimiter.\n",
    "    words = text.split()\n",
    "\n",
    "    # This line uses a list comprehension to filter out stopwords.\n",
    "    # It iterates through the words list and keeps only those words that are not present in the English stopwords list provided by NLTK.\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    #  This line uses another list comprehension to perform stemming.\n",
    "    # It iterates through the filtered words and applies the stem() method of the PorterStemmer to each word, reducing it to its root form.\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#if data has not been processed\n",
    "if not os.path.exists(\"data/processed/\"):\n",
    "        os.makedirs(\"data/processed/\")\n",
    "\n",
    "        train_set_processed = []\n",
    "        for entry in train_set:\n",
    "                train_set_processed.append(review_to_words(entry))\n",
    "\n",
    "        val_set_processed = []\n",
    "        for entry in val_set:\n",
    "                val_set_processed.append(review_to_words(entry))\n",
    "\n",
    "        test_set_processed = []\n",
    "        for entry in test_set:\n",
    "                test_set_processed.append(review_to_words(entry))\n",
    "\n",
    "        # Save into a JSON file\n",
    "        with open('data/processed/train_set_processed.json', 'w') as f:\n",
    "                json.dump(train_set_processed, open('data/processed/train_set_processed.json', 'w'))\n",
    "        with open('data/processed/test_set_processed.json', 'w') as f:\n",
    "                json.dump(test_set_processed, open('data/processed/test_set_processed.json', 'w'))\n",
    "        with open('data/processed/val_set_processed.json', 'w') as f:\n",
    "                json.dump(val_set_processed, open('data/processed/val_set_processed.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3089d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kenneth', 'branagh', 'show', 'excel', 'skill', 'act', 'write', 'deep', 'thought', 'provok', 'interpret', 'shakespear', 'classic', 'well', 'written', 'tragedi', 'kenneth', 'play', 'role', 'hamlet', 'distinct', 'emot', 'provok', 'tear', 'kate', 'winslet', 'perform', 'also', 'great', 'note']\n"
     ]
    }
   ],
   "source": [
    "# Open and load preprocessed data\n",
    "f=open('data/processed/test_set_processed.json')\n",
    "test_set_processed = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f=open('data/processed/train_set_processed.json')\n",
    "train_set_processed = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f=open('data/processed/val_set_processed.json')\n",
    "val_set_processed = json.load(f)\n",
    "f.close()\n",
    "\n",
    "#display preprocessed data sample\n",
    "print(train_set_processed[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
