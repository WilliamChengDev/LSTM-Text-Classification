{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a5a9d5",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f51930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "if not os.path.exists(\"data/aclImdb_v1.tar.gz\"):\n",
    "        #download database\n",
    "        print(\"downloading database...\")\n",
    "        urllib.request.urlretrieve(\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \"data/aclImdb_v1.tar.gz\")\n",
    "        print(\"download complete\")\n",
    "\n",
    "if not os.path.exists(\"data/aclImdb/\"):\n",
    "        #extract database\n",
    "        print(\"extracting database...\")\n",
    "        with tarfile.open(\"data/aclImdb_v1.tar.gz\", \"r:gz\") as tar:\n",
    "                tar.extractall(path=\"data\")\n",
    "        print(\"database extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa68431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "     # Loop over the two splits: training and testing\n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "\n",
    "        # Loop over both sentiment categories: positive and negative\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "\n",
    "            # Construct path to all text files of the current split and sentiment\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "\n",
    "            # Read each review text file\n",
    "            for f in files:\n",
    "                with open(f, encoding=\"utf-8\") as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Assign label 1 for 'pos' and 0 for 'neg'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "\n",
    "            # Sanity check: ensure that every text has a matching label\n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74031e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDb reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f38e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if not os.path.exists(\"data/data.json\"):\n",
    "        # Save the loaded review texts into a JSON file\n",
    "        json.dump(data, open('data/data.json', 'w'))\n",
    "        # Save the sentiment labels into another JSON file\n",
    "        json.dump(labels, open('data/labels.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb500d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and load the movie review data from 'data.json'\n",
    "f=open('data/data.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "# Open and load the sentiment labels from 'labels.json'\n",
    "f=open('data/labels.json')\n",
    "labels = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbd8882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "print(\"IMDb reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27afbbb",
   "metadata": {},
   "source": [
    "# 1. Understanding the Data\n",
    "function that gets the average, maximum, and minimum word length of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1147b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understanding_data(data, set):\n",
    "        maxlength = 0\n",
    "        minlength = 99999999999999999999999\n",
    "        total = 0\n",
    "\n",
    "        for sentiment in data[set]:\n",
    "                for entry in data[set][sentiment]:\n",
    "                        word_count = len(entry.split())\n",
    "                        total += word_count\n",
    "                        if word_count > maxlength:\n",
    "                                maxlength = word_count\n",
    "                        if word_count < minlength:\n",
    "                                minlength = word_count\n",
    "        set_size = len(data[set]['pos'])+len(data[set]['neg'])\n",
    "        print(\"{} set avg word count: {}, max word count: {}, min word count {}\".format(set, total/set_size, maxlength, minlength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f110d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set avg word count: 233.7872, max word count: 2470, min word count 10\n",
      "test set avg word count: 228.52668, max word count: 2278, min word count 4\n"
     ]
    }
   ],
   "source": [
    "understanding_data(data, \"train\")\n",
    "understanding_data(data, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58448b0",
   "metadata": {},
   "source": [
    "#### Word Length Results:\n",
    "| | avg word count| max word count | min word count|\n",
    "|---|---|---|---|\n",
    "|train set| 233.7872 | 2470 | 10|\n",
    "|test set | 229.52688 | 2278 | 4|\n",
    "\n",
    "The word count of the reviews seem to vary quite drastically, with the longest review being almost 2500 words,\\\n",
    "and the shortest being only 4, and an average of about 230 words. Since the LSTM model reads one word at at time,\\\n",
    "reviews that are very long (~2500 words) might make training take significantly more time/memory, while on the\\\n",
    "other hand reviews that are very short (<10 words) probably won't give the model enough context to train on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341960e8",
   "metadata": {},
   "source": [
    "# 2. Creating a Balanced Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b1e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def split_data(data, labels, val_size = 3000):\n",
    "        pos_data, pos_labels = shuffle(data['train']['pos'], labels['train']['pos'], random_state=420) #keeps the shuffling consistent\n",
    "        neg_data, neg_labels = shuffle(data['train']['neg'], labels['train']['neg'], random_state=420)\n",
    "\n",
    "        val_size_pos = int(val_size/2)\n",
    "        val_size_neg = val_size - val_size_pos\n",
    "\n",
    "        train_set = pos_data[val_size_pos:] + neg_data[val_size_neg:]\n",
    "\n",
    "        val_set = pos_data[:val_size_pos] + neg_data[:val_size_neg]\n",
    "\n",
    "        test_set = data['test']['pos'] + data['test']['neg']\n",
    "\n",
    "        print(\"Train: {} | Val: {} | Test: {}\".format(\n",
    "                len(train_set), len(val_set), len(test_set)\n",
    "        ))\n",
    "        print(\"IMDB data: train = pos {} / neg {} , val = pos {} / neg {}, test = pos {} / neg {}\".format(\n",
    "                len(pos_data) - val_size_pos, len(neg_data) - val_size_neg, \n",
    "                val_size_pos, val_size_neg, \n",
    "                len(data['test']['pos']), len(data['test']['neg'])\n",
    "        ))\n",
    "\n",
    "        train_labels = pos_labels[val_size_pos:] + neg_labels[val_size_neg:]\n",
    "        val_labels = pos_labels[:val_size_pos] + neg_labels[:val_size_neg]\n",
    "        test_labels = labels['test']['pos'] + labels['test']['neg']\n",
    "\n",
    "        return train_set, val_set, test_set, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de884148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 22000 | Val: 3000 | Test: 25000\n",
      "IMDB data: train = pos 11000 / neg 11000 , val = pos 1500 / neg 1500, test = pos 12500 / neg 12500\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, test_set, train_labels, val_labels, test_labels = split_data(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750eea24",
   "metadata": {},
   "source": [
    "# 3.  Data Preprocessing\n",
    "- Convert text to lowercase\n",
    "- Remove punctuation\n",
    "- Tokenize text\n",
    "- Remove stop words\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa1d3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line imports the Natural Language Toolkit (NLTK) library, which provides various tools and resources for working with human language data.\n",
    "import nltk\n",
    "\n",
    "# This imports the stopwords module from NLTK, which contains a list of common English words (like \"the\", \"a\", \"is\")\n",
    "# that often don't carry significant meaning in text analysis and are usually removed.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# This imports the PorterStemmer class from NLTK.\n",
    "# Stemming is the process of reducing words to their root or base form (e.g., \"running\" becomes \"run\").\n",
    "# The Porter stemmer is a widely used algorithm for this purpose.\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# This imports the re module, which provides support for regular expressions. Regular expressions are powerful tools for pattern matching and text manipulation.\n",
    "import re\n",
    "\n",
    "#  This imports the BeautifulSoup library, which is used for parsing HTML and XML documents. It's helpful for extracting text content from web pages or documents that might contain HTML tags.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# This defines a function named review_to_words that takes a single argument review, which is expected to be a string containing the text of a movie review.\n",
    "def review_to_words(review):\n",
    "    # This line downloads the list of stopwords from NLTK if it hasn't been downloaded already. The quiet=True argument suppresses the download output.\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "    # This creates an instance of the PorterStemmer class, which we'll use later for stemming words.\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # This line uses BeautifulSoup to parse the input review as an HTML document (\"html.parser\" specifies the parser to use).\n",
    "    # Then, .get_text() extracts the visible text content, effectively removing any HTML tags that might be present in the review.\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "\n",
    "    # text.lower(): It converts the entire text to lowercase. This ensures that words like \"The\" and \"the\" are treated as the same.\n",
    "    # re.sub(r\"[^a-zA-Z0-9]\", \" \", ...): It replaces any character that is not an uppercase letter (a-z), a lowercase letter (A-Z), or a digit (0-9) with a space.\n",
    "    # This helps in removing punctuation marks and other special characters.\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    # This line splits the processed text into a list of individual words using whitespace as the delimiter.\n",
    "    words = text.split()\n",
    "\n",
    "    # This line uses a list comprehension to filter out stopwords.\n",
    "    # It iterates through the words list and keeps only those words that are not present in the English stopwords list provided by NLTK.\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    #  This line uses another list comprehension to perform stemming.\n",
    "    # It iterates through the filtered words and applies the stem() method of the PorterStemmer to each word, reducing it to its root form.\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21c29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#if data has not been processed\n",
    "if not os.path.exists(\"data/processed/\"):\n",
    "        os.makedirs(\"data/processed/\")\n",
    "\n",
    "        train_set_processed = []\n",
    "        for entry in train_set:\n",
    "                train_set_processed.append(review_to_words(entry))\n",
    "\n",
    "        val_set_processed = []\n",
    "        for entry in val_set:\n",
    "                val_set_processed.append(review_to_words(entry))\n",
    "\n",
    "        test_set_processed = []\n",
    "        for entry in test_set:\n",
    "                test_set_processed.append(review_to_words(entry))\n",
    "\n",
    "        # Save into a JSON file\n",
    "        with open('data/processed/train_set_processed.json', 'w') as f:\n",
    "                json.dump(train_set_processed, open('data/processed/train_set_processed.json', 'w'))\n",
    "        with open('data/processed/test_set_processed.json', 'w') as f:\n",
    "                json.dump(test_set_processed, open('data/processed/test_set_processed.json', 'w'))\n",
    "        with open('data/processed/val_set_processed.json', 'w') as f:\n",
    "                json.dump(val_set_processed, open('data/processed/val_set_processed.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e189d8",
   "metadata": {},
   "source": [
    "# 4. Implementing an LSTM Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3089d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seen', 'movi', 'must', 'read', 'three', 'thor', 'heyerdahl', 'book', 'kon', 'tiki', 'ra', 'aku', 'aku', 'activ', 'look', 'copi', 'movi', 'thesi', 'peruvian', 'migrat', 'polynesia', 'aliv', 'well', 'consid', 'crew', 'gp', 'old', 'fashion', 'valv', 'tube', 'radio', '6', 'watt', 'output', 'voyag', 'heroic', 'say', 'least', 'pleas', 'repli', 'messag', 'tell', 'locat', 'copi', 'video', 'would', 'interest', 'buy']\n"
     ]
    }
   ],
   "source": [
    "# Open and load preprocessed data\n",
    "f=open('data/processed/test_set_processed.json')\n",
    "test_set_processed = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f=open('data/processed/train_set_processed.json')\n",
    "train_set_processed = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f=open('data/processed/val_set_processed.json')\n",
    "val_set_processed = json.load(f)\n",
    "f.close()\n",
    "\n",
    "#display preprocessed data sample\n",
    "print(train_set_processed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1047d121",
   "metadata": {},
   "source": [
    "### Build word_dict \\+ Tokenize\n",
    "\n",
    "to tokenize the data into integers for the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d425e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "\n",
    "    # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    word_count = {}\n",
    "\n",
    "    # This outer loop iterates through each sentence (which is a list of words) in the input data.\n",
    "    # The inner loop iterates through each word within the current sentence\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "\n",
    "    # Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    # sorted_words[-1] is the least frequently appearing word.\n",
    "\n",
    "    sorted_words = sorted(word_count, key=word_count.get, reverse=True)\n",
    "\n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2 # 'infrequent' labels\n",
    "\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5a98622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movi': 2, 'film': 3, 'one': 4, 'like': 5, 'time': 6, 'good': 7, 'make': 8, 'charact': 9, 'get': 10, 'see': 11, 'watch': 12, 'stori': 13, 'even': 14, 'would': 15, 'realli': 16, 'well': 17, 'scene': 18, 'look': 19, 'show': 20, 'much': 21, 'end': 22, 'go': 23, 'peopl': 24, 'bad': 25, 'great': 26, 'also': 27, 'first': 28, 'love': 29, 'think': 30, 'way': 31, 'act': 32, 'play': 33, 'made': 34, 'thing': 35, 'could': 36, 'know': 37, 'say': 38, 'seem': 39, 'work': 40, 'plot': 41, 'two': 42, 'year': 43, 'actor': 44, 'seen': 45, 'come': 46, 'mani': 47, 'take': 48, 'life': 49, 'want': 50, 'never': 51, 'littl': 52, 'tri': 53, 'best': 54, 'ever': 55, 'man': 56, 'give': 57, 'better': 58, 'still': 59, 'perform': 60, 'find': 61, 'feel': 62, 'part': 63, 'back': 64, 'someth': 65, 'actual': 66, 'use': 67, 'director': 68, 'interest': 69, 'lot': 70, 'real': 71, 'old': 72, 'cast': 73, 'though': 74, 'live': 75, 'star': 76, 'enjoy': 77, 'guy': 78, 'new': 79, 'noth': 80, 'music': 81, '10': 82, 'anoth': 83, 'role': 84, 'funni': 85, 'point': 86, 'start': 87, 'set': 88, 'girl': 89, 'origin': 90, 'day': 91, 'world': 92, 'everi': 93, 'believ': 94, 'turn': 95, 'quit': 96, 'thought': 97, 'direct': 98, 'us': 99, 'fact': 100, 'horror': 101, 'minut': 102, 'comedi': 103, 'kill': 104, 'around': 105, 'action': 106, 'pretti': 107, 'young': 108, 'happen': 109, 'right': 110, 'wonder': 111, 'got': 112, 'effect': 113, 'howev': 114, 'long': 115, 'line': 116, 'big': 117, 'enough': 118, 'seri': 119, 'famili': 120, 'need': 121, 'fan': 122, 'may': 123, 'bit': 124, 'script': 125, 'without': 126, 'beauti': 127, 'becom': 128, 'person': 129, 'reason': 130, 'tell': 131, 'friend': 132, 'alway': 133, 'kid': 134, 'saw': 135, 'must': 136, 'final': 137, 'least': 138, 'last': 139, 'put': 140, 'almost': 141, 'done': 142, 'whole': 143, 'sure': 144, 'place': 145, 'kind': 146, 'complet': 147, 'differ': 148, 'shot': 149, 'mean': 150, 'far': 151, 'expect': 152, 'anyth': 153, 'sinc': 154, 'book': 155, 'begin': 156, 'might': 157, 'laugh': 158, 'name': 159, 'probabl': 160, '2': 161, 'help': 162, 'let': 163, 'woman': 164, 'call': 165, 'screen': 166, 'entertain': 167, 'read': 168, 'away': 169, 'moment': 170, 'tv': 171, 'yet': 172, 'rather': 173, 'fun': 174, 'worst': 175, 'hard': 176, 'run': 177, 'lead': 178, 'episod': 179, 'audienc': 180, 'idea': 181, 'found': 182, 'anyon': 183, 'appear': 184, 'american': 185, 'although': 186, 'bore': 187, 'hope': 188, 'anim': 189, 'especi': 190, 'cours': 191, 'keep': 192, 'goe': 193, 'job': 194, 'sens': 195, 'move': 196, 'version': 197, 'war': 198, 'dvd': 199, 'money': 200, 'someon': 201, 'true': 202, 'everyth': 203, 'nice': 204, 'second': 205, 'three': 206, 'mayb': 207, 'mind': 208, 'recommend': 209, 'problem': 210, 'hous': 211, 'follow': 212, 'product': 213, 'rate': 214, 'leav': 215, 'night': 216, 'worth': 217, 'main': 218, 'special': 219, 'human': 220, 'sound': 221, 'togeth': 222, '1': 223, 'excel': 224, 'face': 225, 'hand': 226, 'wast': 227, 'eye': 228, 'everyon': 229, 'review': 230, 'father': 231, 'later': 232, 'boy': 233, 'hour': 234, 'john': 235, 'view': 236, 'instead': 237, 'high': 238, 'said': 239, 'wife': 240, 'classic': 241, 'talk': 242, 'miss': 243, 'understand': 244, 'black': 245, 'half': 246, 'left': 247, 'head': 248, 'open': 249, 'care': 250, 'write': 251, 'chang': 252, 'death': 253, 'murder': 254, 'rememb': 255, 'viewer': 256, 'die': 257, 'short': 258, 'surpris': 259, 'less': 260, 'els': 261, 'fight': 262, 'gener': 263, 'fall': 264, 'entir': 265, 'includ': 266, 'involv': 267, 'home': 268, 'attempt': 269, 'total': 270, 'piec': 271, 'simpli': 272, 'usual': 273, 'budget': 274, 'hollywood': 275, 'top': 276, 'pictur': 277, 'power': 278, 'releas': 279, 'men': 280, 'suppos': 281, 'song': 282, 'terribl': 283, 'possibl': 284, '3': 285, 'stupid': 286, 'disappoint': 287, 'camera': 288, 'video': 289, 'low': 290, 'portray': 291, 'featur': 292, 'coupl': 293, 'produc': 294, 'either': 295, 'dead': 296, 'definit': 297, 'aw': 298, 'wrong': 299, 'except': 300, 'poor': 301, 'given': 302, 'absolut': 303, 'women': 304, 'word': 305, 'rest': 306, 'lack': 307, 'perfect': 308, 'titl': 309, 'writer': 310, 'talent': 311, 'truli': 312, 'along': 313, 'style': 314, 'decid': 315, 'full': 316, 'close': 317, 'age': 318, 'school': 319, 'sex': 320, 'save': 321, 'comment': 322, 'next': 323, 'emot': 324, 'case': 325, 'bring': 326, 'killer': 327, 'mr': 328, 'joke': 329, 'came': 330, 'sort': 331, 'creat': 332, 'perhap': 333, 'small': 334, 'heart': 335, 'art': 336, 'base': 337, 'brother': 338, 'flick': 339, 'dialogu': 340, 'game': 341, 'sever': 342, 'written': 343, 'meet': 344, 'earli': 345, 'mother': 346, 'humor': 347, 'consid': 348, 'sequenc': 349, 'develop': 350, 'actress': 351, 'often': 352, 'dark': 353, 'guess': 354, 'other': 355, 'amaz': 356, 'felt': 357, 'white': 358, 'exampl': 359, 'lost': 360, 'stop': 361, 'light': 362, 'cinema': 363, 'drama': 364, 'present': 365, 'imagin': 366, 'children': 367, 'experi': 368, 'unfortun': 369, 'hit': 370, 'forc': 371, 'mention': 372, 'ye': 373, 'manag': 374, 'natur': 375, 'ask': 376, 'son': 377, 'support': 378, 'qualiti': 379, 'fail': 380, 'impress': 381, 'cut': 382, 'went': 383, 'voic': 384, 'wors': 385, 'certainli': 386, 'car': 387, 'stand': 388, 'extrem': 389, 'oh': 390, 'overal': 391, 'side': 392, 'evil': 393, 'danc': 394, 'number': 395, 'basic': 396, 'type': 397, 'favorit': 398, 'mysteri': 399, 'wait': 400, 'alreadi': 401, '5': 402, 'horribl': 403, 'hero': 404, 'despit': 405, 'learn': 406, 'michael': 407, 'genr': 408, 'matter': 409, 'zombi': 410, '4': 411, 'walk': 412, 'fine': 413, 'throughout': 414, 'success': 415, 'town': 416, 'daughter': 417, 'histori': 418, 'realiz': 419, 'b': 420, 'child': 421, 'relationship': 422, 'past': 423, 'wish': 424, 'question': 425, 'late': 426, 'theme': 427, 'hate': 428, 'annoy': 429, 'citi': 430, 'today': 431, 'sometim': 432, 'credit': 433, 'event': 434, 'pleas': 435, 'behind': 436, 'twist': 437, 'god': 438, 'stay': 439, 'sit': 440, 'deal': 441, 'touch': 442, 'rent': 443, 'blood': 444, 'abl': 445, 'comic': 446, 'soon': 447, 'appar': 448, 'anyway': 449, 'deserv': 450, 'edit': 451, 'brilliant': 452, 'chanc': 453, 'bodi': 454, 'slow': 455, 'incred': 456, 'major': 457, 'gave': 458, 'etc': 459, 'level': 460, 'score': 461, 'figur': 462, 'stuff': 463, 'element': 464, 'self': 465, 'situat': 466, 'return': 467, 'group': 468, 'obvious': 469, 'order': 470, 'ridicul': 471, 'happi': 472, 'thank': 473, 'dream': 474, 'highli': 475, 'decent': 476, 'ladi': 477, 'continu': 478, 'add': 479, 'pace': 480, 'novel': 481, 'speak': 482, 'strang': 483, 'shoot': 484, 'took': 485, 'career': 486, 'husband': 487, 'sad': 488, 'break': 489, 'violenc': 490, 'pain': 491, 'polic': 492, 'cannot': 493, 'particularli': 494, 'heard': 495, 'season': 496, 'pick': 497, 'told': 498, 'import': 499, 'recent': 500, 'strong': 501, 'countri': 502, 'robert': 503, 'predict': 504, 'hilari': 505, 'documentari': 506, 'result': 507, 'obviou': 508, 'serious': 509, 'opinion': 510, 'gore': 511, 'crap': 512, 'hell': 513, 'realiti': 514, 'critic': 515, 'offer': 516, 'compar': 517, 'known': 518, 'state': 519, 'visual': 520, 'theater': 521, 'hear': 522, 'effort': 523, 'hold': 524, 'alon': 525, 'explain': 526, 'jame': 527, 'clich': 528, 'thriller': 529, 'exist': 530, 'local': 531, 'ago': 532, 'rock': 533, 'room': 534, 'caus': 535, 'allow': 536, 'femal': 537, 'king': 538, 'sequel': 539, 'none': 540, 'david': 541, 'sister': 542, 'note': 543, 'suspens': 544, 'class': 545, 'sexual': 546, 'oscar': 547, 'huge': 548, 'cool': 549, 'ok': 550, 'simpl': 551, 'similar': 552, 'deliv': 553, 'avoid': 554, 'excit': 555, 'seriou': 556, 'buy': 557, 'provid': 558, 'taken': 559, 'valu': 560, 'convinc': 561, 'win': 562, 'shock': 563, 'english': 564, 'apart': 565, 'scari': 566, 'cinematographi': 567, 'shown': 568, 'polit': 569, 'exactli': 570, 'check': 571, 'filmmak': 572, 'street': 573, 'spoiler': 574, 'middl': 575, 'across': 576, 'tale': 577, 'whose': 578, 'somewhat': 579, 'form': 580, 'pass': 581, 'offic': 582, 'modern': 583, 'sing': 584, 'carri': 585, 'silli': 586, 'confus': 587, 'richard': 588, 'messag': 589, 'mostli': 590, 'stage': 591, 'cop': 592, 'parent': 593, 'charm': 594, 'subject': 595, 'attent': 596, 'earth': 597, 'singl': 598, 'team': 599, 'jack': 600, 'monster': 601, 'five': 602, 'prove': 603, 'unlik': 604, 'marri': 605, 'villain': 606, 'cover': 607, 'four': 608, 'dialog': 609, 'futur': 610, 'televis': 611, 'adult': 612, 'georg': 613, 'william': 614, 'toward': 615, 'respect': 616, '8': 617, '7': 618, 'cheap': 619, 'pay': 620, 'due': 621, 'pull': 622, 'remind': 623, 'paul': 624, 'atmospher': 625, 'typic': 626, 'fill': 627, 'escap': 628, 'crime': 629, 'artist': 630, 'detail': 631, 'dog': 632, 'gun': 633, 'british': 634, 'drive': 635, 'date': 636, '80': 637, 'non': 638, 'knew': 639, 'fast': 640, 'intellig': 641, 'build': 642, 'peter': 643, 'clearli': 644, 'easili': 645, 'romant': 646, 'doubt': 647, 'weak': 648, 'member': 649, 'whether': 650, 'discov': 651, 'fit': 652, 'captur': 653, 'remain': 654, 'fire': 655, 'straight': 656, 'aspect': 657, 'imag': 658, 'beyond': 659, 'mark': 660, 'plan': 661, 'attack': 662, 'match': 663, 'appreci': 664, 'near': 665, 'fantast': 666, 'air': 667, 'upon': 668, 'adapt': 669, 'posit': 670, 'mari': 671, 'period': 672, 'ten': 673, 'nearli': 674, 'dull': 675, 'within': 676, 'red': 677, 'inspir': 678, 'york': 679, 'materi': 680, 'realist': 681, 'mess': 682, 'color': 683, 'clear': 684, 'box': 685, 'lose': 686, 'spend': 687, 'busi': 688, 'chase': 689, 'battl': 690, 'space': 691, 'suffer': 692, 'storylin': 693, 'forget': 694, 'easi': 695, 'finish': 696, 'bunch': 697, 'truth': 698, 'lee': 699, 'victim': 700, 'de': 701, 'western': 702, 'train': 703, 'e': 704, 'notic': 705, 'among': 706, 'standard': 707, 'eventu': 708, 'normal': 709, 'french': 710, 'spirit': 711, 'teenag': 712, 'tom': 713, 'drug': 714, 'accept': 715, 'soldier': 716, 'ad': 717, 'third': 718, 'design': 719, 'dramat': 720, 'list': 721, 'agre': 722, 'larg': 723, '9': 724, 'bill': 725, 'cri': 726, 'ultim': 727, 'sorri': 728, 'cartoon': 729, 'adventur': 730, 'babi': 731, 'suggest': 732, 'soundtrack': 733, 'troubl': 734, 'premis': 735, 'attract': 736, 'contain': 737, 'romanc': 738, 'cultur': 739, 'mix': 740, 'somehow': 741, 'famou': 742, 'rare': 743, 'secret': 744, 'certain': 745, 'reveal': 746, 'kept': 747, 'disney': 748, 'gone': 749, 'fantasi': 750, 'lame': 751, 'throw': 752, 'greatest': 753, 'suck': 754, 'fear': 755, 'wit': 756, 'copi': 757, 'pure': 758, 'shame': 759, 'america': 760, 'scare': 761, 'appeal': 762, 'warn': 763, '20': 764, 'male': 765, 'inde': 766, 'master': 767, 'harri': 768, 'alien': 769, 'brought': 770, 'issu': 771, 'screenplay': 772, 'treat': 773, 'admit': 774, 'averag': 775, 'whatev': 776, 'locat': 777, 'weird': 778, 'particular': 779, 'relat': 780, 'student': 781, 'award': 782, 'struggl': 783, 'background': 784, 'okay': 785, 'hot': 786, 'poorli': 787, 'memor': 788, '70': 789, 'describ': 790, 'free': 791, 'forward': 792, 'societi': 793, 'odd': 794, 'imdb': 795, 'water': 796, '30': 797, 'studio': 798, 'project': 799, 'difficult': 800, 'doctor': 801, 'remak': 802, 'magic': 803, 'potenti': 804, 'becam': 805, 'costum': 806, 'amus': 807, 'fiction': 808, 'dr': 809, 'masterpiec': 810, 'express': 811, 'accent': 812, 'uniqu': 813, 'japanes': 814, 'crazi': 815, 'prison': 816, 'unless': 817, 'scream': 818, 'choic': 819, 'vampir': 820, 'fli': 821, 'roll': 822, 'lover': 823, 'control': 824, 'refer': 825, 'creepi': 826, 'execut': 827, 'cheesi': 828, 'jump': 829, 'superb': 830, 'jane': 831, 'wood': 832, 'joe': 833, 'wear': 834, 'fi': 835, 'footag': 836, 'depict': 837, 'sci': 838, 'public': 839, 'flaw': 840, 'ghost': 841, 'otherwis': 842, 'week': 843, 'plu': 844, 'badli': 845, 'plenti': 846, 'bother': 847, 'deep': 848, 'parti': 849, 'moral': 850, 'girlfriend': 851, 'outsid': 852, 'older': 853, 'earlier': 854, 'connect': 855, 'maker': 856, 'grow': 857, 'popular': 858, 'disturb': 859, '90': 860, 'quickli': 861, 'sweet': 862, 'social': 863, 'concept': 864, 'c': 865, 'equal': 866, 'combin': 867, 'stick': 868, 'band': 869, 'mistak': 870, 'mad': 871, 'surviv': 872, 'eat': 873, 'ride': 874, 'perfectli': 875, 'lie': 876, 'gay': 877, 'cat': 878, 'co': 879, 'dress': 880, 'catch': 881, 'beat': 882, 'dumb': 883, 'era': 884, 'previou': 885, 'stereotyp': 886, 'meant': 887, 'answer': 888, 'christma': 889, 'rich': 890, 'concern': 891, 'serv': 892, 'listen': 893, 'term': 894, 'tone': 895, 'bare': 896, 'front': 897, 'inform': 898, 'insid': 899, 'cute': 900, 'post': 901, 'law': 902, 'compani': 903, 'la': 904, 'desper': 905, 'fairli': 906, 'hardli': 907, 'variou': 908, 'flat': 909, 'scott': 910, 'hair': 911, 'store': 912, 'track': 913, '50': 914, 'centuri': 915, 'kick': 916, 'cold': 917, 'memori': 918, 'intent': 919, 'plain': 920, 'sadli': 921, 'land': 922, 'innoc': 923, 'remark': 924, 'brain': 925, 'steal': 926, 'promis': 927, 'amount': 928, 'danger': 929, 'engag': 930, 'german': 931, 'nuditi': 932, 'crew': 933, 'claim': 934, 'spot': 935, 'histor': 936, 'rip': 937, 'record': 938, 'creatur': 939, 'univers': 940, 'tension': 941, 'intens': 942, 'share': 943, 'suit': 944, 'travel': 945, 'toni': 946, 'player': 947, 'ben': 948, 'tast': 949, 'destroy': 950, 'sleep': 951, 'purpos': 952, 'step': 953, 'familiar': 954, 'depth': 955, 'violent': 956, 'ignor': 957, 'wrote': 958, 'languag': 959, 'delight': 960, 'island': 961, 'achiev': 962, 'unbeliev': 963, 'fascin': 964, 'detect': 965, 'teen': 966, 'liter': 967, 'manner': 968, 'collect': 969, 'door': 970, 'ruin': 971, 'suddenli': 972, 'approach': 973, 'reveng': 974, 'trip': 975, 'christian': 976, 'italian': 977, 'commun': 978, 'burn': 979, 'fashion': 980, 'caught': 981, 'scienc': 982, 'sick': 983, 'neither': 984, 'clever': 985, 'ann': 986, 'physic': 987, 'intrigu': 988, 'reach': 989, 'channel': 990, 'abil': 991, 'limit': 992, 'introduc': 993, 'slightli': 994, 'trash': 995, 'comput': 996, 'nation': 997, 'bizarr': 998, 'rape': 999, 'million': 1000, 'imposs': 1001, 'hole': 1002, 'crimin': 1003, 'extra': 1004, 'skill': 1005, 'soul': 1006, 'immedi': 1007, 'paint': 1008, 'complex': 1009, 'embarrass': 1010, 'mere': 1011, 'mental': 1012, 'fake': 1013, 'ring': 1014, '6': 1015, 'camp': 1016, 'haunt': 1017, 'conclus': 1018, 'indian': 1019, 'edg': 1020, 'dad': 1021, 'blue': 1022, 'suspect': 1023, 'planet': 1024, 'focu': 1025, 'search': 1026, 'faith': 1027, 'key': 1028, 'spent': 1029, 'drag': 1030, 'pop': 1031, 'laughabl': 1032, 'slasher': 1033, 'common': 1034, 'biggest': 1035, 'drop': 1036, 'younger': 1037, 'receiv': 1038, 'rais': 1039, 'technic': 1040, 'arriv': 1041, 'tear': 1042, 'angel': 1043, '15': 1044, 'solid': 1045, 'awesom': 1046, 'former': 1047, 'handl': 1048, 'f': 1049, 'stun': 1050, 'count': 1051, 'colleg': 1052, 'hurt': 1053, 'respons': 1054, 'super': 1055, 'desir': 1056, 'visit': 1057, 'smith': 1058, 'genuin': 1059, 'heavi': 1060, 'pointless': 1061, 'pathet': 1062, 'tough': 1063, 'van': 1064, 'davi': 1065, 'grade': 1066, 'motion': 1067, 'tortur': 1068, 'excus': 1069, 'fair': 1070, 'brutal': 1071, 'author': 1072, 'chemistri': 1073, 'sign': 1074, 'exploit': 1075, 'explor': 1076, 'intend': 1077, 'obsess': 1078, 'addit': 1079, 'cult': 1080, 'wall': 1081, 'cross': 1082, 'consist': 1083, 'bar': 1084, 'cloth': 1085, '60': 1086, 'somewher': 1087, 'compel': 1088, 'frank': 1089, 'commit': 1090, 'focus': 1091, 'regard': 1092, 'steve': 1093, 'grant': 1094, 'dub': 1095, 'boss': 1096, 'minor': 1097, 'park': 1098, 'depress': 1099, 'trailer': 1100, 'jim': 1101, 'tradit': 1102, 'besid': 1103, 'gang': 1104, 'longer': 1105, 'south': 1106, 'creativ': 1107, 'rule': 1108, 'race': 1109, 'road': 1110, 'green': 1111, 'g': 1112, 'anti': 1113, 'u': 1114, 'surprisingli': 1115, 'ground': 1116, '40': 1117, 'silent': 1118, 'opportun': 1119, 'scientist': 1120, 'motiv': 1121, 'asid': 1122, 'draw': 1123, 'thrill': 1124, 'decad': 1125, 'west': 1126, 'narr': 1127, 'mood': 1128, 'wild': 1129, 'display': 1130, 'armi': 1131, 'govern': 1132, 'nobodi': 1133, 'redeem': 1134, 'judg': 1135, 'club': 1136, 'festiv': 1137, 'london': 1138, 'serial': 1139, 'humour': 1140, 'sam': 1141, 'mouth': 1142, 'honestli': 1143, 'utterli': 1144, 'repeat': 1145, 'idiot': 1146, 'ship': 1147, 'cost': 1148, 'honest': 1149, 'loos': 1150, 'nowher': 1151, 'journey': 1152, 'report': 1153, 'giant': 1154, 'investig': 1155, 'smile': 1156, 'sexi': 1157, 'noir': 1158, 'ex': 1159, 'militari': 1160, 'enter': 1161, 'practic': 1162, 'marriag': 1163, 'geniu': 1164, 'stewart': 1165, 'aliv': 1166, 'machin': 1167, '100': 1168, 'henri': 1169, 'glad': 1170, 'demon': 1171, 'area': 1172, 'yeah': 1173, 'admir': 1174, 'folk': 1175, 'bear': 1176, 'page': 1177, 'garbag': 1178, 'climax': 1179, 'stone': 1180, 'terrif': 1181, 'center': 1182, 'bought': 1183, 'kelli': 1184, 'hang': 1185, 'tire': 1186, 'loud': 1187, 'graphic': 1188, 'requir': 1189, 'occasion': 1190, 'agent': 1191, 'hors': 1192, 'charli': 1193, 'passion': 1194, 'doubl': 1195, 'r': 1196, 'profession': 1197, 'studi': 1198, 'nightmar': 1199, 'send': 1200, 'industri': 1201, 'nake': 1202, 'frame': 1203, 'drawn': 1204, 'damn': 1205, 'chri': 1206, 'insult': 1207, 'wise': 1208, 'gem': 1209, 'christoph': 1210, 'seek': 1211, 'affect': 1212, 'arm': 1213, 'l': 1214, 'boyfriend': 1215, 'wow': 1216, 'church': 1217, 'rel': 1218, 'subtl': 1219, 'opera': 1220, 'affair': 1221, 'grace': 1222, 'blow': 1223, 'outstand': 1224, 'presenc': 1225, 'cinemat': 1226, 'system': 1227, 'conflict': 1228, 'constantli': 1229, 'initi': 1230, 'hotel': 1231, 'essenti': 1232, 'avail': 1233, 'challeng': 1234, 'jone': 1235, 'fresh': 1236, 'thu': 1237, 'central': 1238, 'gag': 1239, 'fulli': 1240, 'wind': 1241, 'satisfi': 1242, 'sell': 1243, 'sceneri': 1244, 'flashback': 1245, 'smart': 1246, 'everybodi': 1247, 'repres': 1248, 'iron': 1249, 'month': 1250, 'bottom': 1251, 'narrat': 1252, 'assum': 1253, 'allen': 1254, 'slowli': 1255, 'hide': 1256, 'impact': 1257, 'bed': 1258, 'hire': 1259, 'evid': 1260, 'content': 1261, 'interview': 1262, 'witch': 1263, 'hunt': 1264, 'ray': 1265, 'adam': 1266, 'batman': 1267, 'absurd': 1268, 'born': 1269, 'pari': 1270, 'sight': 1271, 'j': 1272, 'phone': 1273, 'model': 1274, 'justic': 1275, 'individu': 1276, 'thrown': 1277, 'photographi': 1278, 'lesson': 1279, 'charl': 1280, 'rise': 1281, 'neg': 1282, 'nomin': 1283, 'ill': 1284, 'fool': 1285, 'push': 1286, 'sent': 1287, 'zero': 1288, 'tragedi': 1289, 'fellow': 1290, 'independ': 1291, 'resembl': 1292, 'mediocr': 1293, 'bomb': 1294, 'bond': 1295, 'hey': 1296, 'pack': 1297, 'program': 1298, 'sub': 1299, 'angl': 1300, 'choos': 1301, 'summer': 1302, 'strike': 1303, 'likabl': 1304, 'brief': 1305, 'skip': 1306, 'hospit': 1307, 'logic': 1308, 'occur': 1309, 'discuss': 1310, 'cameo': 1311, 'billi': 1312, 'held': 1313, 'shine': 1314, 'abus': 1315, 'queen': 1316, 'ahead': 1317, 'spoil': 1318, 'vision': 1319, 'hill': 1320, 'current': 1321, 'protagonist': 1322, 'six': 1323, 'photograph': 1324, 'blame': 1325, 'jean': 1326, 'mainli': 1327, 'trust': 1328, 'blond': 1329, 'satir': 1330, 'twice': 1331, 'encount': 1332, 'bruce': 1333, 'lord': 1334, 'trap': 1335, 'field': 1336, 'martin': 1337, 'drink': 1338, 'round': 1339, 'teacher': 1340, 'plane': 1341, 'ball': 1342, 'thin': 1343, 'object': 1344, 'reaction': 1345, 'commerci': 1346, 'influenc': 1347, 'tragic': 1348, 'surround': 1349, 'robot': 1350, 'mile': 1351, 'prefer': 1352, 'nonsens': 1353, 'pleasur': 1354, 'vote': 1355, 'porn': 1356, 'ii': 1357, 'station': 1358, 'tie': 1359, 'random': 1360, 'segment': 1361, 'highlight': 1362, 'join': 1363, 'worthi': 1364, 'gorgeou': 1365, 'intern': 1366, 'will': 1367, 'mom': 1368, 'improv': 1369, 'psycholog': 1370, 'brian': 1371, '11': 1372, 'youth': 1373, 'theatr': 1374, 'superior': 1375, 'singer': 1376, 'funniest': 1377, 'sport': 1378, 'epic': 1379, 'nasti': 1380, 'jerri': 1381, 'prepar': 1382, 'tape': 1383, 'heaven': 1384, 'legend': 1385, 'forev': 1386, 'ugli': 1387, 'jackson': 1388, 'childhood': 1389, 'roger': 1390, 'dollar': 1391, 'dread': 1392, 'fortun': 1393, 'favourit': 1394, 'convers': 1395, 'latter': 1396, 'progress': 1397, 'warm': 1398, 'wide': 1399, 'sky': 1400, 'fell': 1401, 'crash': 1402, 'gangster': 1403, 'castl': 1404, 'seven': 1405, 'ident': 1406, 'shop': 1407, 'establish': 1408, 'paid': 1409, 'unusu': 1410, 'fox': 1411, 'formula': 1412, 'buddi': 1413, 'celebr': 1414, 'devil': 1415, 'andi': 1416, 'mine': 1417, 'pilot': 1418, 'supposedli': 1419, 'disast': 1420, 'al': 1421, 'length': 1422, 'p': 1423, 'rob': 1424, 'tend': 1425, 'suicid': 1426, 'mask': 1427, 'stephen': 1428, 'reflect': 1429, 'stuck': 1430, 'that': 1431, 'disgust': 1432, 'clean': 1433, 'danni': 1434, 'disappear': 1435, 'wed': 1436, 'tune': 1437, 'forgotten': 1438, 'partner': 1439, 'unit': 1440, 'seemingli': 1441, 'convey': 1442, 'heroin': 1443, 'replac': 1444, 'recogn': 1445, 'arthur': 1446, 'pair': 1447, 'desert': 1448, 'thoroughli': 1449, 'accur': 1450, 'frustrat': 1451, 'decis': 1452, 'remot': 1453, 'ms': 1454, 'market': 1455, 'european': 1456, 'clue': 1457, 'therefor': 1458, 'fault': 1459, 'irrit': 1460, 'instanc': 1461, 'trick': 1462, 'onto': 1463, 'jason': 1464, 'readi': 1465, 'afraid': 1466, 'ed': 1467, 'refus': 1468, 'captain': 1469, 'eddi': 1470, 'test': 1471, 'river': 1472, 'angri': 1473, 'food': 1474, 'x': 1475, 'dirti': 1476, 'quick': 1477, 'explan': 1478, 'led': 1479, 'alan': 1480, 'johnni': 1481, 'uncl': 1482, 'rescu': 1483, 'commentari': 1484, 'daniel': 1485, 'worri': 1486, 'bland': 1487, 'wooden': 1488, 'fate': 1489, 'hunter': 1490, 'aim': 1491, 'accid': 1492, 'news': 1493, 'energi': 1494, 'price': 1495, 'piti': 1496, '12': 1497, '1950': 1498, 'martial': 1499, 'n': 1500, 'lock': 1501, 'invent': 1502, 'transform': 1503, 'anymor': 1504, 'rush': 1505, 'favor': 1506, 'insan': 1507, 'process': 1508, 'owner': 1509, 'weapon': 1510, 'hidden': 1511, 'floor': 1512, 'joy': 1513, 'insight': 1514, 'board': 1515, 'comparison': 1516, 'target': 1517, 'cgi': 1518, 'russian': 1519, 'movement': 1520, 'steven': 1521, 'rang': 1522, 'opposit': 1523, 'seat': 1524, 'dougla': 1525, 'england': 1526, 'research': 1527, 'chines': 1528, 'teach': 1529, 'necessari': 1530, 'protect': 1531, 'comed': 1532, 'hint': 1533, 'awar': 1534, 'bank': 1535, 'religi': 1536, 'attitud': 1537, 'grand': 1538, 'rain': 1539, 'window': 1540, 'dick': 1541, 'symbol': 1542, 'deepli': 1543, 'sentiment': 1544, 'whatsoev': 1545, 'possess': 1546, 'pre': 1547, 'friendship': 1548, 'media': 1549, 'terror': 1550, 'credibl': 1551, 'cage': 1552, 'charg': 1553, 'simon': 1554, 'bloodi': 1555, 'mid': 1556, 'ford': 1557, 'shadow': 1558, 'drunk': 1559, 'crowd': 1560, 'accord': 1561, 'leader': 1562, 'freddi': 1563, 'knowledg': 1564, 'anybodi': 1565, 'twenti': 1566, 'breath': 1567, 'empti': 1568, 'chick': 1569, 'mountain': 1570, 'bug': 1571, 'brown': 1572, 'distract': 1573, 'chill': 1574, 'watchabl': 1575, 'flesh': 1576, 'andrew': 1577, 'perspect': 1578, 'princ': 1579, 'appropri': 1580, 'keaton': 1581, 'kiss': 1582, 'soft': 1583, 'tim': 1584, 'account': 1585, 'villag': 1586, 'anywher': 1587, 'enemi': 1588, 'nativ': 1589, 'unknown': 1590, 'began': 1591, 'magnific': 1592, 'load': 1593, 'tarzan': 1594, 'thousand': 1595, 'somebodi': 1596, 'dozen': 1597, 'fred': 1598, 'stretch': 1599, 'ice': 1600, 'dan': 1601, 'kate': 1602, 'utter': 1603, 'radio': 1604, 'blind': 1605, '000': 1606, 'worker': 1607, 'knock': 1608, 'parodi': 1609, 'dare': 1610, 'marvel': 1611, 'taylor': 1612, '1980': 1613, 'japan': 1614, 'jr': 1615, 'interpret': 1616, 'kevin': 1617, 'revolv': 1618, 'quot': 1619, 'nazi': 1620, 'wave': 1621, 'unnecessari': 1622, 'hundr': 1623, 'strength': 1624, 'vh': 1625, 'hitler': 1626, 'explos': 1627, 'shakespear': 1628, 'franc': 1629, '1970': 1630, 'root': 1631, 'nick': 1632, 'convent': 1633, 'prais': 1634, 'frighten': 1635, 'sat': 1636, 'stunt': 1637, 'quiet': 1638, 'correct': 1639, 'speed': 1640, 'privat': 1641, 'contrast': 1642, 'translat': 1643, 'spi': 1644, 'soap': 1645, 'sum': 1646, 'candi': 1647, 'jesu': 1648, 'jeff': 1649, 'academi': 1650, 'regular': 1651, 'leg': 1652, 'stock': 1653, 'recal': 1654, 'behavior': 1655, 'craft': 1656, 'mission': 1657, 'vehicl': 1658, 'technolog': 1659, 'debut': 1660, 'gari': 1661, 'command': 1662, 'mike': 1663, 'notabl': 1664, 'sea': 1665, 'buck': 1666, 'moon': 1667, 'punch': 1668, 'pretenti': 1669, 'ass': 1670, 'lynch': 1671, 'superman': 1672, 'confront': 1673, 'foot': 1674, 'techniqu': 1675, 'cabl': 1676, 'african': 1677, 'threaten': 1678, 'demand': 1679, 'attend': 1680, 'higher': 1681, 'bright': 1682, 'gold': 1683, 'failur': 1684, 'code': 1685, 'civil': 1686, 'separ': 1687, 'jimmi': 1688, 'servic': 1689, 'compet': 1690, 'toy': 1691, 'gotten': 1692, 'winner': 1693, 'realis': 1694, 'vs': 1695, 'presid': 1696, 'finest': 1697, 'gene': 1698, 'spanish': 1699, 'educ': 1700, 'kidnap': 1701, 'determin': 1702, 'broken': 1703, 'structur': 1704, 'capabl': 1705, 'advanc': 1706, 'constant': 1707, 'factor': 1708, 'fat': 1709, 'complic': 1710, 'abandon': 1711, 'numer': 1712, 'morn': 1713, 'up': 1714, 'observ': 1715, 'aid': 1716, 'site': 1717, 'whilst': 1718, 'loss': 1719, 'depart': 1720, 'joan': 1721, 'belong': 1722, 'lone': 1723, 'domin': 1724, 'hook': 1725, 'contriv': 1726, 'belief': 1727, 'foreign': 1728, 'met': 1729, 'freak': 1730, 'bet': 1731, 'complain': 1732, 'secur': 1733, 'dri': 1734, 'paper': 1735, 'smoke': 1736, 'dancer': 1737, 'guard': 1738, 'con': 1739, 'shape': 1740, 'routin': 1741, 'succeed': 1742, 'h': 1743, 'unfunni': 1744, 'witti': 1745, 'speech': 1746, 'santa': 1747, 'peac': 1748, 'oper': 1749, 'albert': 1750, 'hat': 1751, 'w': 1752, 'rank': 1753, 'lewi': 1754, 'luck': 1755, 'fame': 1756, 'virtual': 1757, 'eric': 1758, 'kinda': 1759, 'boat': 1760, 'grab': 1761, 'awkward': 1762, 'rose': 1763, 'tree': 1764, 'shallow': 1765, 'rubbish': 1766, 'corni': 1767, 'washington': 1768, 'v': 1769, 'clark': 1770, 'clip': 1771, 'werewolf': 1772, 'fu': 1773, 'bigger': 1774, 'kong': 1775, 'comfort': 1776, 'context': 1777, '13': 1778, 'jennif': 1779, 'patient': 1780, 'bob': 1781, 'welcom': 1782, 'realism': 1783, 'pretend': 1784, 'flash': 1785, 'flow': 1786, 'manipul': 1787, 'assist': 1788, 'interact': 1789, 'trek': 1790, 'cash': 1791, 'cain': 1792, 'lesbian': 1793, 'whenev': 1794, 'psycho': 1795, 'activ': 1796, 'forgiv': 1797, 'offens': 1798, 'associ': 1799, 'sake': 1800, 'safe': 1801, 'accomplish': 1802, 'mirror': 1803, 'specif': 1804, 'wake': 1805, 'cheat': 1806, 'contribut': 1807, 'wing': 1808, 'sean': 1809, 'core': 1810, 'addict': 1811, 'organ': 1812, 'frequent': 1813, 'ordinari': 1814, 'skin': 1815, 'thoma': 1816, 'dimension': 1817, 'theatric': 1818, 'grew': 1819, 'unexpect': 1820, 'asian': 1821, 'golden': 1822, 'corrupt': 1823, 'advic': 1824, 'religion': 1825, 'anthoni': 1826, 'regret': 1827, 'luke': 1828, 'lake': 1829, 'reli': 1830, 'hank': 1831, 'signific': 1832, 'print': 1833, 'lower': 1834, 'sheriff': 1835, 'devic': 1836, 'pile': 1837, 'degre': 1838, 'accident': 1839, 'curiou': 1840, 'unfold': 1841, 'gain': 1842, 'dislik': 1843, 'howard': 1844, 'colour': 1845, 'grown': 1846, 'lucki': 1847, 'depend': 1848, 'subtitl': 1849, 'construct': 1850, 'crappi': 1851, 'experienc': 1852, 'freedom': 1853, 'sourc': 1854, 'betti': 1855, 'balanc': 1856, 'statu': 1857, 'condit': 1858, 'walter': 1859, 'drew': 1860, 'altern': 1861, 'promot': 1862, 'frankli': 1863, 'statement': 1864, 'gift': 1865, 'meanwhil': 1866, 'card': 1867, 'path': 1868, 'sole': 1869, 'invit': 1870, 'veteran': 1871, 'gonna': 1872, 'morgan': 1873, 'emerg': 1874, 'liner': 1875, 'intellectu': 1876, 'menac': 1877, 'sudden': 1878, 'mass': 1879, 'matur': 1880, 'gordon': 1881, 'contemporari': 1882, 'jacki': 1883, 'halloween': 1884, 'unabl': 1885, 'defin': 1886, 'gratuit': 1887, 'oliv': 1888, 'amateur': 1889, 'anna': 1890, 'grip': 1891, 'authent': 1892, 'winter': 1893, 'theori': 1894, 'fare': 1895, 'sheer': 1896, 'fish': 1897, 'overli': 1898, 'sinatra': 1899, 'treasur': 1900, 'woodi': 1901, 'overlook': 1902, 'brilliantli': 1903, 'categori': 1904, 'footbal': 1905, 'lawyer': 1906, 'brook': 1907, 'victoria': 1908, 'priest': 1909, 'hall': 1910, 'honor': 1911, 'crack': 1912, 'underr': 1913, 'driven': 1914, 'bird': 1915, 'broadway': 1916, 'chief': 1917, 'gross': 1918, 'sensit': 1919, 'surreal': 1920, 'cheer': 1921, 'spectacular': 1922, 'section': 1923, 'virgin': 1924, 'glass': 1925, 'robin': 1926, 'stranger': 1927, 'cook': 1928, 'sympathet': 1929, 'built': 1930, 'treatment': 1931, 'fatal': 1932, 'neighbor': 1933, 'matt': 1934, 'le': 1935, 'ape': 1936, 'k': 1937, 'wayn': 1938, 'ancient': 1939, 'unintent': 1940, 'demonstr': 1941, 'ran': 1942, 'endless': 1943, 'captiv': 1944, 'circumst': 1945, 'laughter': 1946, 'canadian': 1947, 'letter': 1948, 'wound': 1949, 'cowboy': 1950, 'solv': 1951, 'spell': 1952, 'par': 1953, 'topic': 1954, 'comedian': 1955, 'consequ': 1956, 'com': 1957, 'handsom': 1958, 'kung': 1959, 'scenario': 1960, 'michel': 1961, 'feet': 1962, 'blockbust': 1963, 'consider': 1964, 'compos': 1965, 'scale': 1966, 'expos': 1967, 'unrealist': 1968, 'monkey': 1969, 'substanc': 1970, 'curs': 1971, 'remov': 1972, 'wander': 1973, 'relief': 1974, 'anger': 1975, 'switch': 1976, 'spoof': 1977, 'chan': 1978, 'moor': 1979, 'barbara': 1980, 'powel': 1981, 'germani': 1982, 'inevit': 1983, 'bullet': 1984, 'chosen': 1985, 'roy': 1986, 'edward': 1987, 'ryan': 1988, 'nanci': 1989, 'advertis': 1990, 'brave': 1991, 'stumbl': 1992, 'strongli': 1993, 'earn': 1994, 'alex': 1995, 'gut': 1996, 'screenwrit': 1997, 'convict': 1998, 'gori': 1999, 'patrick': 2000, 'prevent': 2001, 'wrap': 2002, 'juli': 2003, 'refresh': 2004, '1930': 2005, 'victor': 2006, 'network': 2007, 'liber': 2008, 'assassin': 2009, 'proper': 2010, 'loui': 2011, 'parker': 2012, 'loser': 2013, 'bat': 2014, 'indic': 2015, 'expert': 2016, 'vagu': 2017, 'pleasant': 2018, 'nevertheless': 2019, 'slap': 2020, 'sharp': 2021, 'willi': 2022, 'deni': 2023, 'amateurish': 2024, 'offend': 2025, 'ador': 2026, 'murphi': 2027, 'africa': 2028, 'terrifi': 2029, 'harsh': 2030, 'anderson': 2031, 'miser': 2032, 'trial': 2033, 'joseph': 2034, 'rental': 2035, 'eight': 2036, 'reput': 2037, 'dragon': 2038, 'invis': 2039, 'naiv': 2040, 'uk': 2041, 'australian': 2042, 'creator': 2043, 'saturday': 2044, 'outrag': 2045, 'descript': 2046, 'crude': 2047, 'alcohol': 2048, 'provok': 2049, 'supernatur': 2050, 'justifi': 2051, 'cynic': 2052, 'europ': 2053, 'mini': 2054, 'uninterest': 2055, 'russel': 2056, 'complaint': 2057, 'dinosaur': 2058, 'cell': 2059, 'bridg': 2060, 'resid': 2061, 'tabl': 2062, 'purchas': 2063, '14': 2064, 'till': 2065, 'rival': 2066, 'hitchcock': 2067, 'sympathi': 2068, 'tediou': 2069, 'imit': 2070, 'hype': 2071, 'san': 2072, 'strip': 2073, 'foster': 2074, 'professor': 2075, 'columbo': 2076, 'bite': 2077, 'semi': 2078, 'dig': 2079, 'che': 2080, 'sword': 2081, 'employ': 2082, 'surfac': 2083, 'plagu': 2084, 'court': 2085, 'defeat': 2086, 'grave': 2087, 'glimps': 2088, 'method': 2089, 'beach': 2090, 'basebal': 2091, 'fals': 2092, 'subplot': 2093, 'ideal': 2094, 'jungl': 2095, 'north': 2096, '0': 2097, 'kim': 2098, 'spin': 2099, 'lion': 2100, 'titan': 2101, 'ludicr': 2102, 'endur': 2103, 'cooper': 2104, 'tini': 2105, 'stare': 2106, 'format': 2107, 'indi': 2108, 'ga': 2109, 'nois': 2110, 'expens': 2111, 'sin': 2112, 'fairi': 2113, 'weekend': 2114, 'driver': 2115, 'melodrama': 2116, 'lousi': 2117, 'holm': 2118, 'excess': 2119, 'guest': 2120, 'nail': 2121, 'upset': 2122, 'task': 2123, 'argu': 2124, 'reminisc': 2125, 'heck': 2126, 'cant': 2127, 'twin': 2128, 'massiv': 2129, 'nude': 2130, 'ala': 2131, 'controversi': 2132, 'closer': 2133, 'erot': 2134, 'alic': 2135, 'trilog': 2136, 'border': 2137, 'accus': 2138, 'press': 2139, 'risk': 2140, 'prime': 2141, 'guid': 2142, 'atroci': 2143, 'warrior': 2144, 'cinderella': 2145, 'princess': 2146, 'character': 2147, 'particip': 2148, 'asleep': 2149, 'inner': 2150, 'aunt': 2151, 'reject': 2152, 'presum': 2153, 'emma': 2154, 'birth': 2155, 'advis': 2156, 'bourn': 2157, 'destruct': 2158, 'lugosi': 2159, 'texa': 2160, 'prostitut': 2161, 'bu': 2162, 'revel': 2163, 'storytel': 2164, 'pitch': 2165, 'popul': 2166, 'max': 2167, 'merit': 2168, 'damag': 2169, 'previous': 2170, 'chose': 2171, 'hip': 2172, 'forest': 2173, 'prior': 2174, 'matrix': 2175, 'dude': 2176, 'sullivan': 2177, 'rachel': 2178, 'environ': 2179, 'forth': 2180, 'corner': 2181, 'mate': 2182, 'clair': 2183, 'propaganda': 2184, 'warner': 2185, 'beast': 2186, 'blah': 2187, 'gather': 2188, 'maintain': 2189, 'urban': 2190, 'exagger': 2191, 'thirti': 2192, 'defend': 2193, 'pacino': 2194, 'confid': 2195, 'bound': 2196, 'medic': 2197, 'courag': 2198, 'widow': 2199, 'wilson': 2200, 'makeup': 2201, 'ton': 2202, 'irish': 2203, 'arrest': 2204, 'sitcom': 2205, 'un': 2206, 'settl': 2207, 'select': 2208, 'shut': 2209, 'examin': 2210, 'citizen': 2211, 'breast': 2212, 'lift': 2213, 'contact': 2214, 'contest': 2215, 'jess': 2216, 'doll': 2217, 'directli': 2218, 'behav': 2219, 'goal': 2220, 'inept': 2221, 'california': 2222, 'notch': 2223, 'metal': 2224, '1960': 2225, 'identifi': 2226, '1990': 2227, 'betray': 2228, 'buff': 2229, 'insist': 2230, 'rat': 2231, 'shake': 2232, 'campi': 2233, 'hood': 2234, 'whoever': 2235, 'hardi': 2236, 'sincer': 2237, 'friday': 2238, 'size': 2239, 'latest': 2240, 'exact': 2241, 'poster': 2242, 'donald': 2243, 'concentr': 2244, 'philip': 2245, 'host': 2246, 'jaw': 2247, 'forgett': 2248, 'blend': 2249, 'moron': 2250, 'friendli': 2251, 'pet': 2252, 'ear': 2253, 'corps': 2254, 'branagh': 2255, 'hoffman': 2256, 'multipl': 2257, 'varieti': 2258, 'stolen': 2259, 'shower': 2260, 'attach': 2261, 'accompani': 2262, 'sir': 2263, 'pan': 2264, 'deadli': 2265, 'rough': 2266, 'mgm': 2267, 'aka': 2268, 'duke': 2269, 'icon': 2270, 'doom': 2271, 'gritti': 2272, 'borrow': 2273, 'sidney': 2274, 'offici': 2275, 'instal': 2276, 'unconvinc': 2277, 'leagu': 2278, 'quirki': 2279, '25': 2280, 'crush': 2281, 'worthwhil': 2282, 'afternoon': 2283, 'mainstream': 2284, 'screw': 2285, 'shark': 2286, 'denni': 2287, 'yell': 2288, 'pitt': 2289, 'grim': 2290, 'sink': 2291, 'confess': 2292, 'sun': 2293, 'storm': 2294, 'slapstick': 2295, 'guilti': 2296, 'sunday': 2297, 'proud': 2298, 'swim': 2299, 'quest': 2300, 'legendari': 2301, 'lifetim': 2302, 'sentenc': 2303, 'truck': 2304, 'overcom': 2305, 'link': 2306, 'dean': 2307, 'summari': 2308, 'lisa': 2309, 'bell': 2310, 'lip': 2311, 'tight': 2312, 'satan': 2313, 'vietnam': 2314, 'nose': 2315, 'mexican': 2316, 'split': 2317, 'stomach': 2318, 'ned': 2319, 'entri': 2320, 'spite': 2321, 'revolut': 2322, 'toler': 2323, 'landscap': 2324, 'jerk': 2325, 'reward': 2326, 'terrorist': 2327, 'alright': 2328, 'racist': 2329, 'buri': 2330, 'bag': 2331, 'scheme': 2332, 'fade': 2333, 'pursu': 2334, 'deliveri': 2335, 'sarah': 2336, 'hong': 2337, 'johnson': 2338, 'mechan': 2339, 'cousin': 2340, 'tremend': 2341, 'tour': 2342, 'everywher': 2343, 'feed': 2344, 'ted': 2345, 'thumb': 2346, 'chain': 2347, 'teeth': 2348, 'finger': 2349, 'fabul': 2350, 'pro': 2351, 'distinct': 2352, 'downright': 2353, 'argument': 2354, 'china': 2355, 'lincoln': 2356, 'creation': 2357, 'junk': 2358, 'notori': 2359, 'outfit': 2360, 'relev': 2361, 'larri': 2362, 'retard': 2363, 'render': 2364, 'traci': 2365, 'occas': 2366, 'hugh': 2367, 'proceed': 2368, 'flight': 2369, 'susan': 2370, 'sandler': 2371, 'julia': 2372, 'garden': 2373, 'hyster': 2374, 'blown': 2375, 'survivor': 2376, 'divorc': 2377, 'christ': 2378, 'gruesom': 2379, 'greater': 2380, 'elizabeth': 2381, 'bone': 2382, 'turkey': 2383, 'chair': 2384, 'jail': 2385, 'string': 2386, 'extraordinari': 2387, 'facial': 2388, 'stab': 2389, 'amazingli': 2390, 'corpor': 2391, 'benefit': 2392, 'pose': 2393, 'beg': 2394, 'trade': 2395, 'magazin': 2396, 'text': 2397, 'lesli': 2398, 'swear': 2399, 'forgot': 2400, 'fourth': 2401, 'heavili': 2402, 'concert': 2403, 'lazi': 2404, 'cabin': 2405, 'deeper': 2406, 'christi': 2407, 'indulg': 2408, 'tap': 2409, 'mexico': 2410, 'imageri': 2411, 'bride': 2412, 'homosexu': 2413, 'raw': 2414, 'bollywood': 2415, 'stanley': 2416, 'vincent': 2417, 'lane': 2418, 'underground': 2419, 'resist': 2420, 'afterward': 2421, 'bbc': 2422, 'understood': 2423, 'necessarili': 2424, 'incid': 2425, 'holiday': 2426, 'oppos': 2427, '2000': 2428, 'seagal': 2429, 'horrif': 2430, 'india': 2431, 'halfway': 2432, 'tongu': 2433, 'princip': 2434, 'anticip': 2435, 'obscur': 2436, 'belov': 2437, 'obnoxi': 2438, 'von': 2439, 'stalk': 2440, 'subsequ': 2441, 'carol': 2442, 'lay': 2443, 'cruel': 2444, 'steel': 2445, 'photo': 2446, 'fay': 2447, 'burt': 2448, 'punish': 2449, 'maria': 2450, 'cring': 2451, 'address': 2452, '17': 2453, 'conceiv': 2454, 'enhanc': 2455, 'pregnant': 2456, 'guarante': 2457, 'funnier': 2458, 'freeman': 2459, 'via': 2460, 'explod': 2461, 'devot': 2462, 'ha': 2463, 'slave': 2464, 'mob': 2465, 'encourag': 2466, 'overact': 2467, 'overwhelm': 2468, 'integr': 2469, 'capit': 2470, 'snake': 2471, 'elev': 2472, 'jon': 2473, 'introduct': 2474, 'error': 2475, 'blair': 2476, 'ticket': 2477, 'measur': 2478, 'appli': 2479, 'deliber': 2480, 'lili': 2481, 'advantag': 2482, 'grey': 2483, 'relax': 2484, 'fix': 2485, 'disbelief': 2486, 'mous': 2487, 'ron': 2488, 'dynam': 2489, 'bitter': 2490, 'shoe': 2491, 'properli': 2492, 'immens': 2493, 'extent': 2494, 'east': 2495, 'chop': 2496, 'inspector': 2497, 'restor': 2498, 'carpent': 2499, 'basi': 2500, 'brando': 2501, 'inhabit': 2502, 'upper': 2503, 'nowaday': 2504, 'dentist': 2505, 'alter': 2506, 'usa': 2507, 'pool': 2508, 'ensu': 2509, 'stink': 2510, 'bo': 2511, 'bett': 2512, 'isol': 2513, 'southern': 2514, 'miller': 2515, 'navi': 2516, 'circl': 2517, 'chees': 2518, 'everyday': 2519, 'daili': 2520, 'sacrific': 2521, 'nonetheless': 2522, 'transfer': 2523, 'wreck': 2524, 'hippi': 2525, 'prop': 2526, 'nine': 2527, 'asham': 2528, 'rage': 2529, 'block': 2530, 'shirt': 2531, 'jay': 2532, 'spiritu': 2533, 'nurs': 2534, 'zone': 2535, 'digit': 2536, 'reed': 2537, 'massacr': 2538, 'sutherland': 2539, 'savag': 2540, '2001': 2541, 'jessica': 2542, 'lyric': 2543, 'extend': 2544, 'lesser': 2545, 'pound': 2546, 'broadcast': 2547, 'brad': 2548, 'beer': 2549, 'suitabl': 2550, '2006': 2551, 'sleazi': 2552, 'breathtak': 2553, 'succe': 2554, 'uncomfort': 2555, 'user': 2556, 'distribut': 2557, 'goofi': 2558, '1940': 2559, 'widmark': 2560, 'creep': 2561, 'sophist': 2562, 'showcas': 2563, 'infam': 2564, 'impli': 2565, 'portrait': 2566, 'pie': 2567, 'mildli': 2568, 'silver': 2569, 'preview': 2570, 'retir': 2571, 'sinist': 2572, 'lo': 2573, 'chuck': 2574, 'midnight': 2575, '16': 2576, 'contract': 2577, 'etern': 2578, 'laura': 2579, 'plant': 2580, 'fallen': 2581, 'em': 2582, 'baker': 2583, 'shift': 2584, 'react': 2585, 'melt': 2586, 'competit': 2587, 'assign': 2588, 'cinematograph': 2589, 'bulli': 2590, 'horrifi': 2591, 'streisand': 2592, 'diamond': 2593, 'racism': 2594, 'laid': 2595, 'coher': 2596, 'wash': 2597, 'access': 2598, 'knife': 2599, 'innov': 2600, 'miscast': 2601, 'bath': 2602, 'albeit': 2603, 'spring': 2604, 'mill': 2605, 'mindless': 2606, 'nut': 2607, 'reunion': 2608, 'stylish': 2609, 'disguis': 2610, 'absorb': 2611, 'ironi': 2612, 'reduc': 2613, 'aspir': 2614, 'spoken': 2615, 'endear': 2616, 'precis': 2617, 'repetit': 2618, 'stanwyck': 2619, 'cathol': 2620, 'appal': 2621, 'nobl': 2622, 'bang': 2623, 'luci': 2624, 'sold': 2625, 'alik': 2626, 'duti': 2627, 'spielberg': 2628, 'musician': 2629, 'mansion': 2630, 'snow': 2631, 'internet': 2632, 'karloff': 2633, 'uninspir': 2634, 'butt': 2635, 'cruis': 2636, 'oddli': 2637, 'row': 2638, 'thug': 2639, 'jewish': 2640, 'neighborhood': 2641, 'thief': 2642, 'matthau': 2643, 'needless': 2644, 'henc': 2645, 'blank': 2646, 'own': 2647, 'horrend': 2648, 'tower': 2649, 'rochest': 2650, '18': 2651, 'diseas': 2652, 'wwii': 2653, 'illustr': 2654, 'elabor': 2655, 'eeri': 2656, 'amongst': 2657, 'neck': 2658, 'poignant': 2659, 'brand': 2660, 'pride': 2661, 'tool': 2662, 'flop': 2663, 'shoulder': 2664, 'announc': 2665, 'conclud': 2666, 'fulfil': 2667, 'wanna': 2668, 'silenc': 2669, 'scarecrow': 2670, 'stress': 2671, 'adopt': 2672, 'rebel': 2673, 'homeless': 2674, 'wealthi': 2675, 'torn': 2676, 'proof': 2677, 'oil': 2678, 'per': 2679, 'neat': 2680, 'whale': 2681, 'burton': 2682, 'happili': 2683, 'muppet': 2684, 'franchis': 2685, 'rap': 2686, 'britain': 2687, 'document': 2688, 'fighter': 2689, 'function': 2690, 'stan': 2691, 'jake': 2692, 'rocket': 2693, 'crisi': 2694, 'korean': 2695, 'wilder': 2696, 'dear': 2697, 'arrang': 2698, 'derek': 2699, 'dane': 2700, 'st': 2701, 'factori': 2702, 'throat': 2703, 'incoher': 2704, 'vacat': 2705, 'helen': 2706, 'barri': 2707, 'dedic': 2708, 'greatli': 2709, 'triumph': 2710, 'persona': 2711, 'pig': 2712, 'puppet': 2713, 'forti': 2714, 'hamlet': 2715, 'toilet': 2716, 'fest': 2717, 'fanci': 2718, 'spare': 2719, 'meaning': 2720, 'premier': 2721, 'phillip': 2722, 'disagre': 2723, 'heat': 2724, 'weight': 2725, 'trite': 2726, 'cannib': 2727, 'boot': 2728, 'polish': 2729, 'chaplin': 2730, 'caricatur': 2731, 'ambigu': 2732, 'mar': 2733, 'ensembl': 2734, 'spike': 2735, 'pat': 2736, 'journalist': 2737, 'fbi': 2738, 'shout': 2739, 'minim': 2740, 'homag': 2741, 'dinner': 2742, 'philosoph': 2743, 'dump': 2744, 'ward': 2745, 'ritter': 2746, 'cancel': 2747, 'territori': 2748, 'parallel': 2749, 'kurt': 2750, 'difficulti': 2751, 'chuckl': 2752, 'march': 2753, 'ego': 2754, 'engin': 2755, 'ultra': 2756, 'nelson': 2757, 'redempt': 2758, 'resolut': 2759, 'babe': 2760, 'miik': 2761, 'inexplic': 2762, 'profound': 2763, 'tens': 2764, 'carter': 2765, 'itali': 2766, 'grate': 2767, 'wolf': 2768, 'swing': 2769, 'glori': 2770, 'surf': 2771, 'transit': 2772, 'fianc': 2773, 'doc': 2774, 'muslim': 2775, 'elvi': 2776, 'arrog': 2777, 'ban': 2778, 'lloyd': 2779, 'cure': 2780, 'adequ': 2781, 'glover': 2782, 'lust': 2783, 'unforgett': 2784, 'scoobi': 2785, '1st': 2786, 'climb': 2787, 'pg': 2788, 'gadget': 2789, 'catherin': 2790, 'cusack': 2791, 'portion': 2792, 'exercis': 2793, 'bobbi': 2794, 'poison': 2795, 'slip': 2796, '24': 2797, 'denzel': 2798, 'threat': 2799, 'stiff': 2800, '2005': 2801, 'bibl': 2802, 'orient': 2803, 'unbear': 2804, 'file': 2805, 'fifti': 2806, 'height': 2807, 'broke': 2808, 'charisma': 2809, 'sensibl': 2810, 'plastic': 2811, 'dawn': 2812, 'reader': 2813, 'wherea': 2814, 'ham': 2815, 'skit': 2816, 'enorm': 2817, 'financi': 2818, 'cube': 2819, 'viru': 2820, 'choreograph': 2821, 'bush': 2822, 'boredom': 2823, 'tag': 2824, 'boll': 2825, 'peak': 2826, 'luca': 2827, 'lovabl': 2828, 'blast': 2829, 'newspap': 2830, 'wannab': 2831, 'canada': 2832, 'eleph': 2833, 'curti': 2834, 'owe': 2835, 'essenc': 2836, 'guilt': 2837, 'jet': 2838, 'health': 2839, 'medium': 2840, 'bud': 2841, 'arnold': 2842, 'slaughter': 2843, 'mst3k': 2844, 'multi': 2845, 'urg': 2846, 'montag': 2847, 'induc': 2848, 'implaus': 2849, 'useless': 2850, 'kubrick': 2851, 'backdrop': 2852, 'companion': 2853, 'drown': 2854, 'secretari': 2855, 'walken': 2856, 'birthday': 2857, 'float': 2858, 'sneak': 2859, 'simpson': 2860, 'deriv': 2861, 'superfici': 2862, 'reynold': 2863, 'tribut': 2864, 'bakshi': 2865, 'afford': 2866, 'huh': 2867, 'reev': 2868, 'item': 2869, 'electr': 2870, 'puzzl': 2871, 'metaphor': 2872, 'vega': 2873, 'larger': 2874, 'basement': 2875, 'jeremi': 2876, 'restaur': 2877, 'shi': 2878, 'conspiraci': 2879, 'luckili': 2880, 'elvira': 2881, 'godfath': 2882, 'pit': 2883, 'fetch': 2884, 'punk': 2885, 'assur': 2886, 'grandmoth': 2887, 'salli': 2888, 'gate': 2889, 'distanc': 2890, 'gradual': 2891, 'briefli': 2892, 'resourc': 2893, 'subtleti': 2894, 'librari': 2895, 'flynn': 2896, 'editor': 2897, 'increasingli': 2898, 'hawk': 2899, 'kitchen': 2900, 'tender': 2901, 'heston': 2902, 'pad': 2903, 'label': 2904, 'nuclear': 2905, 'invest': 2906, 'notion': 2907, 'assault': 2908, 'countless': 2909, 'spread': 2910, 'resort': 2911, 'hammer': 2912, 'layer': 2913, 'coach': 2914, 'cole': 2915, 'elsewher': 2916, 'importantli': 2917, 'maniac': 2918, 'iii': 2919, 'charismat': 2920, 'shall': 2921, '2002': 2922, 'selfish': 2923, 'eighti': 2924, 'coincid': 2925, 'alli': 2926, 'estat': 2927, 'toss': 2928, 'stood': 2929, 'union': 2930, 'button': 2931, 'updat': 2932, 'hack': 2933, 'monk': 2934, 'ralph': 2935, 'laurel': 2936, 'brosnan': 2937, 'gothic': 2938, 'outcom': 2939, 'divers': 2940, 'stronger': 2941, 'slight': 2942, 'seventi': 2943, 'emili': 2944, 'meat': 2945, 'eastwood': 2946, 'servant': 2947, 'dorothi': 2948, 'resolv': 2949, 'spark': 2950, 'kane': 2951, 'conveni': 2952, 'farm': 2953, 'elect': 2954, 'smooth': 2955, 'evok': 2956, 'closet': 2957, 'vari': 2958, 'vulner': 2959, 'disc': 2960, 'communist': 2961, 'exchang': 2962, 'hudson': 2963, 'eva': 2964, 'piano': 2965, 'suspend': 2966, 'regardless': 2967, 'samurai': 2968, 'superhero': 2969, 'pressur': 2970, 'dealt': 2971, 'neil': 2972, 'sloppi': 2973, '45': 2974, 'insert': 2975, 'dash': 2976, 'sketch': 2977, 'dave': 2978, 'admittedli': 2979, 'worthless': 2980, 'squar': 2981, 'inconsist': 2982, 'lab': 2983, 'doo': 2984, 'struck': 2985, 'jar': 2986, 'grasp': 2987, 'cardboard': 2988, 'pin': 2989, 'drivel': 2990, 'revers': 2991, 'matthew': 2992, 'poverti': 2993, 'recov': 2994, 'instinct': 2995, 'seed': 2996, 'alexand': 2997, 'recreat': 2998, 'cup': 2999, 'ian': 3000, 'discoveri': 3001, 'jami': 3002, 'convolut': 3003, 'lou': 3004, 'rex': 3005, 'qualifi': 3006, 'soviet': 3007, 'ellen': 3008, 'felix': 3009, 'walker': 3010, 'nuanc': 3011, 'we': 3012, 'superbl': 3013, '20th': 3014, 'declar': 3015, 'seduc': 3016, 'cia': 3017, 'tea': 3018, 'tame': 3019, 'clock': 3020, 'fond': 3021, 'ingredi': 3022, 'eleg': 3023, 'kudo': 3024, 'eccentr': 3025, 'mild': 3026, 'spooki': 3027, 'reel': 3028, 'mafia': 3029, 'rave': 3030, 'heroic': 3031, 'splendid': 3032, 'directori': 3033, 'timeless': 3034, 'eas': 3035, 'australia': 3036, 'pleasantli': 3037, 'flip': 3038, 'brillianc': 3039, 'mann': 3040, 'abc': 3041, 'distant': 3042, 'flower': 3043, 'cox': 3044, 'pal': 3045, 'automat': 3046, 'hamilton': 3047, 'easier': 3048, 'mel': 3049, 'splatter': 3050, 'instantli': 3051, 'sadist': 3052, 'miracl': 3053, 'ramon': 3054, 'joey': 3055, 'aforement': 3056, 'scratch': 3057, 'akshay': 3058, 'timothi': 3059, 'astair': 3060, 'overdon': 3061, 'strictli': 3062, 'butcher': 3063, 'burst': 3064, 'websit': 3065, 'perri': 3066, 'terri': 3067, 'bleak': 3068, 'falk': 3069, 'ninja': 3070, 'ken': 3071, 'cave': 3072, 'rid': 3073, 'flawless': 3074, 'bash': 3075, 'psychiatrist': 3076, 'smash': 3077, 'pauli': 3078, 'shortli': 3079, 'mildr': 3080, 'wrestl': 3081, 'repeatedli': 3082, 'empir': 3083, 'farc': 3084, 'cagney': 3085, 'dracula': 3086, 'clown': 3087, 'ho': 3088, 'beaten': 3089, 'anni': 3090, 'incompet': 3091, 'slice': 3092, 'ambiti': 3093, 'hop': 3094, 'sidekick': 3095, 'dawson': 3096, 'craven': 3097, 'melodramat': 3098, 'fonda': 3099, 'carrey': 3100, 'fx': 3101, 'purpl': 3102, 'goldberg': 3103, 'be': 3104, 'nod': 3105, 'partli': 3106, 'fist': 3107, 'scientif': 3108, 'gray': 3109, 'tall': 3110, 'hatr': 3111, 'harm': 3112, 'ranger': 3113, 'wipe': 3114, 'absenc': 3115, 'chew': 3116, 'lifestyl': 3117, 'web': 3118, 'philosophi': 3119, 'mitchel': 3120, 'craig': 3121, 'futurist': 3122, 'gentl': 3123, 'gotta': 3124, 'cameron': 3125, 'phrase': 3126, 'awe': 3127, 'bless': 3128, 'manhattan': 3129, 'graduat': 3130, 'dire': 3131, 'broad': 3132, 'tank': 3133, 'glenn': 3134, 'favour': 3135, 'destin': 3136, 'elderli': 3137, 'cliff': 3138, 'templ': 3139, 'seller': 3140, 'astonish': 3141, 'mock': 3142, 'turner': 3143, 'darker': 3144, 'min': 3145, 'atlanti': 3146, 'kay': 3147, 'gandhi': 3148, 'cheek': 3149, 'knight': 3150, 'jealou': 3151, 'giallo': 3152, 'explicit': 3153, 'modesti': 3154, 'uniform': 3155, 'margaret': 3156, 'nerv': 3157, '2004': 3158, 'norman': 3159, 'duo': 3160, 'publish': 3161, 'ireland': 3162, 'enthusiast': 3163, 'eve': 3164, 'legal': 3165, 'lend': 3166, 'awaken': 3167, 'panic': 3168, 'sunshin': 3169, 'judi': 3170, 'conserv': 3171, 'saint': 3172, 'increas': 3173, 'artifici': 3174, 'fifteen': 3175, 'psychot': 3176, 'stoog': 3177, 'mummi': 3178, 'kenneth': 3179, 'francisco': 3180, 'foul': 3181, 'natali': 3182, 'evolv': 3183, 'drunken': 3184, 'staff': 3185, 'homicid': 3186, 'unpleas': 3187, 'befriend': 3188, 'thread': 3189, 'contrari': 3190, 'soprano': 3191, 'curios': 3192, 'glow': 3193, 'slightest': 3194, 'overr': 3195, 'blade': 3196, 'collabor': 3197, 'brazil': 3198, 'roman': 3199, 'versu': 3200, 'ami': 3201, 'rivet': 3202, 'warren': 3203, 'debat': 3204, 'pray': 3205, 'wick': 3206, 'bin': 3207, 'antic': 3208, 'pulp': 3209, 'digniti': 3210, 'prom': 3211, 'adolesc': 3212, 'crook': 3213, 'eyr': 3214, 'helicopt': 3215, 'confirm': 3216, 'suppli': 3217, 'li': 3218, 'ambit': 3219, 'fisher': 3220, 'childish': 3221, 'fulci': 3222, 'stiller': 3223, 'romero': 3224, 'ah': 3225, 'prize': 3226, 'beatti': 3227, 'reunit': 3228, 'wizard': 3229, 'revolutionari': 3230, 'album': 3231, 'royal': 3232, 'custom': 3233, 'timon': 3234, 'pierc': 3235, 'detract': 3236, 'forbidden': 3237, 'todd': 3238, 'transport': 3239, 'racial': 3240, 'lemmon': 3241, 'olivi': 3242, 'homer': 3243, 'packag': 3244, 'boast': 3245, 'swedish': 3246, 'hart': 3247, 'chapter': 3248, 'bradi': 3249, 'yesterday': 3250, 'twelv': 3251, 'fund': 3252, 'whip': 3253, 'chao': 3254, 'holi': 3255, 'coloni': 3256, 'wallac': 3257, 'trio': 3258, 'carl': 3259, 'funer': 3260, 'reviv': 3261, 'boil': 3262, 'combat': 3263, 'centr': 3264, 'voyag': 3265, 'delici': 3266, '2007': 3267, 'preciou': 3268, 'politician': 3269, 'choreographi': 3270, 'plausibl': 3271, 'ought': 3272, 'torment': 3273, 'lawrenc': 3274, 'launch': 3275, 'griffith': 3276, 'jazz': 3277, 'respond': 3278, '99': 3279, 'rabbit': 3280, 'infect': 3281, 'kapoor': 3282, 'godzilla': 3283, 'unwatch': 3284, 'mickey': 3285, 'stole': 3286, 'leonard': 3287, 'destini': 3288, 'audit': 3289, 'juvenil': 3290, 'dose': 3291, 'ant': 3292, 'bedroom': 3293, 'vivid': 3294, 'bull': 3295, 'apolog': 3296, 'suspici': 3297, 'palma': 3298, 'abysm': 3299, 'lumet': 3300, 'madonna': 3301, 'bold': 3302, 'popcorn': 3303, 'meaningless': 3304, 'incident': 3305, 'viciou': 3306, 'uneven': 3307, 'bathroom': 3308, 'elimin': 3309, 'cd': 3310, '1996': 3311, 'defi': 3312, 'alert': 3313, 'harvey': 3314, 'flame': 3315, 'prejudic': 3316, 'wire': 3317, 'leap': 3318, 'garner': 3319, 'vast': 3320, 'stinker': 3321, 'kirk': 3322, 'pseudo': 3323, 'ruth': 3324, 'sixti': 3325, 'neglect': 3326, 'domest': 3327, 'mystic': 3328, 'colleagu': 3329, 'principl': 3330, 'enterpris': 3331, 'await': 3332, 'holli': 3333, 'clint': 3334, 'ocean': 3335, 'harder': 3336, 'tribe': 3337, 'pun': 3338, 'leon': 3339, 'invas': 3340, 'cake': 3341, 'kennedi': 3342, 'garbo': 3343, 'nicholson': 3344, 'horrid': 3345, 'donna': 3346, 'altogeth': 3347, 'spit': 3348, 'neo': 3349, 'jenni': 3350, 'tomato': 3351, 'lean': 3352, 'primari': 3353, 'enchant': 3354, 'descend': 3355, 'reserv': 3356, 'greek': 3357, 'blake': 3358, 'wendi': 3359, 'gabriel': 3360, 'promin': 3361, 'synopsi': 3362, 'april': 3363, 'colonel': 3364, 'furthermor': 3365, 'bottl': 3366, 'rambo': 3367, 'mall': 3368, 'tiger': 3369, 'acclaim': 3370, 'gender': 3371, 'gundam': 3372, 'daddi': 3373, 'clumsi': 3374, 'termin': 3375, 'visibl': 3376, 'awak': 3377, 'streep': 3378, 'candid': 3379, 'shirley': 3380, 'immigr': 3381, 'solut': 3382, 'smaller': 3383, 'q': 3384, 'poetic': 3385, 'tip': 3386, '2003': 3387, 'acknowledg': 3388, 'trail': 3389, 'et': 3390, 'yellow': 3391, 'merci': 3392, 'loyal': 3393, 'marshal': 3394, 'museum': 3395, 'pirat': 3396, 'scope': 3397, 'celluloid': 3398, 'exhibit': 3399, 'psychic': 3400, 'solo': 3401, 'threw': 3402, 'devoid': 3403, 'chest': 3404, 'dalton': 3405, 'brooklyn': 3406, 'karen': 3407, 'audio': 3408, 'diana': 3409, 'inherit': 3410, 'bargain': 3411, 'unsettl': 3412, 'dimens': 3413, 'verhoeven': 3414, 'voight': 3415, 'outer': 3416, 'breed': 3417, 'marin': 3418, 'pant': 3419, 'undoubtedli': 3420, 'secondli': 3421, 'shade': 3422, 'glamor': 3423, 'dian': 3424, 'hollow': 3425, 'sore': 3426, 'gear': 3427, 'disjoint': 3428, 'berlin': 3429, 'hideou': 3430, 'thick': 3431, 'retain': 3432, 'unreal': 3433, 'tail': 3434, 'robberi': 3435, 'ration': 3436, 'coffe': 3437, 'interrupt': 3438, 'nerd': 3439, 'tommi': 3440, 'sh': 3441, 'inferior': 3442, 'edi': 3443, 'consum': 3444, 'devast': 3445, 'mistaken': 3446, 'justin': 3447, 'decor': 3448, 'hardcor': 3449, 'bust': 3450, 'inappropri': 3451, 'lol': 3452, 'characterist': 3453, 'poke': 3454, 'amanda': 3455, 'ethnic': 3456, 'pervers': 3457, 'counter': 3458, 'ya': 3459, 'coat': 3460, 'obtain': 3461, 'echo': 3462, 'airplan': 3463, 'option': 3464, 'imperson': 3465, 'altman': 3466, 'exot': 3467, 'rick': 3468, 'dutch': 3469, 'moodi': 3470, 'mail': 3471, 'vanish': 3472, 'grandfath': 3473, 'unhappi': 3474, 'illeg': 3475, 'paus': 3476, 'nearbi': 3477, 'cope': 3478, 'defens': 3479, 'ruthless': 3480, 'embrac': 3481, 'inan': 3482, 'beneath': 3483, 'vulgar': 3484, 'pen': 3485, 'properti': 3486, 'parad': 3487, 'enlighten': 3488, 'spider': 3489, 'stellar': 3490, '3rd': 3491, 'predecessor': 3492, 'pearl': 3493, 'declin': 3494, 'fever': 3495, 'z': 3496, 'slick': 3497, 'polanski': 3498, '2nd': 3499, 'ventur': 3500, 'institut': 3501, 'victori': 3502, 'cuba': 3503, 'blatant': 3504, 'saga': 3505, 'hal': 3506, 'protest': 3507, '1999': 3508, 'primarili': 3509, 'lit': 3510, 'rural': 3511, 'yard': 3512, 'wet': 3513, 'troop': 3514, 'global': 3515, 'repuls': 3516, 'dealer': 3517, 'arab': 3518, 'hartley': 3519, 'trashi': 3520, 'rifl': 3521, 'shelley': 3522, 'hesit': 3523, 'mixtur': 3524, 'niro': 3525, 'farmer': 3526, 'stir': 3527, 'dud': 3528, 'wont': 3529, 'owen': 3530, 'andr': 3531, 'shelf': 3532, 'simplist': 3533, 'talki': 3534, 'experiment': 3535, 'wealth': 3536, 'collaps': 3537, 'olli': 3538, 'incomprehens': 3539, 'tooth': 3540, 'crocodil': 3541, 'snl': 3542, 'riot': 3543, 'oz': 3544, 'similarli': 3545, 'instant': 3546, 'roommat': 3547, 'commend': 3548, 'humbl': 3549, 'pot': 3550, 'robinson': 3551, 'drum': 3552, 'skull': 3553, 'del': 3554, 'woo': 3555, 'bergman': 3556, 'fri': 3557, 'dive': 3558, 'florida': 3559, 'marti': 3560, 'gloriou': 3561, 'slug': 3562, 'dixon': 3563, 'equip': 3564, 'steam': 3565, 'khan': 3566, 'uwe': 3567, 'applaud': 3568, 'hung': 3569, 'ballet': 3570, 'fanat': 3571, 'recruit': 3572, 'rope': 3573, 'disgrac': 3574, 'biographi': 3575, 'goer': 3576, 'porno': 3577, 'heartbreak': 3578, 'bow': 3579, 'blunt': 3580, 'hyde': 3581, 'bumbl': 3582, 'highest': 3583, 'rider': 3584, 'faster': 3585, 'aveng': 3586, 'biko': 3587, 'mayor': 3588, 'myer': 3589, 'topless': 3590, 'vice': 3591, 'pokemon': 3592, 'sale': 3593, 'martian': 3594, 'jew': 3595, 'illog': 3596, 'alfr': 3597, 'rendit': 3598, 'feminist': 3599, 'psychopath': 3600, 'sibl': 3601, 'cari': 3602, 'span': 3603, 'linda': 3604, 'hopeless': 3605, 'gimmick': 3606, 'milk': 3607, 'earl': 3608, 'immort': 3609, 'despair': 3610, 'engross': 3611, 'suffic': 3612, 'scroog': 3613, 'franci': 3614, 'blob': 3615, 'pervert': 3616, 'sandra': 3617, 'jeffrey': 3618, 'whine': 3619, 'sympath': 3620, 'assembl': 3621, 'contempl': 3622, 'emperor': 3623, 'safeti': 3624, 'humili': 3625, 'revolt': 3626, 'craze': 3627, 'maggi': 3628, 'grinch': 3629, 'muddl': 3630, 'uplift': 3631, 'confin': 3632, 'blew': 3633, 'eager': 3634, 'emphasi': 3635, 'partial': 3636, 'hbo': 3637, 'repress': 3638, 'composit': 3639, 'jonathan': 3640, 'tonight': 3641, 'ginger': 3642, 'whoopi': 3643, 'downhil': 3644, 'chicken': 3645, 'nostalg': 3646, 'deed': 3647, 'carradin': 3648, 'dismiss': 3649, 'iran': 3650, 'ensur': 3651, 'maid': 3652, 'speci': 3653, 'boom': 3654, 'slide': 3655, 'soccer': 3656, 'sensat': 3657, 'senseless': 3658, '1983': 3659, '1972': 3660, 'amitabh': 3661, 'repli': 3662, 'shed': 3663, 'abort': 3664, 'cue': 3665, 'ethan': 3666, 'taught': 3667, 'preach': 3668, 'graham': 3669, 'compass': 3670, 'richardson': 3671, 'hopper': 3672, 'paranoia': 3673, 'casual': 3674, 'bend': 3675, 'wive': 3676, 'kyle': 3677, 'trace': 3678, 'mundan': 3679, '35': 3680, 'worm': 3681, 'choru': 3682, 'snap': 3683, 'cg': 3684, 'farrel': 3685, 'pour': 3686, 'plight': 3687, 'recycl': 3688, 'lowest': 3689, 'mistress': 3690, 'abraham': 3691, 'charlott': 3692, 'catchi': 3693, 'bleed': 3694, 'stilt': 3695, 'arguabl': 3696, 'vocal': 3697, 'morri': 3698, 'quarter': 3699, 'da': 3700, 'pattern': 3701, 'hello': 3702, 'tad': 3703, 'der': 3704, 'maci': 3705, 'thru': 3706, 'gap': 3707, 'patriot': 3708, 'percept': 3709, 'vengeanc': 3710, 'resurrect': 3711, 'sissi': 3712, 'frankenstein': 3713, 'zizek': 3714, 'paltrow': 3715, 'rambl': 3716, 'bowl': 3717, 'strain': 3718, 'numb': 3719, 'linger': 3720, 'strand': 3721, 'julian': 3722, 'seduct': 3723, 'trademark': 3724, 'mesmer': 3725, 'dement': 3726, 'weakest': 3727, 'bubbl': 3728, 'fido': 3729, '00': 3730, 'poem': 3731, 'arc': 3732, 'dust': 3733, 'recognit': 3734, 'nervou': 3735, 'sappi': 3736, 'acquir': 3737, 'aggress': 3738, 'intim': 3739, 'domino': 3740, 'compens': 3741, 'mario': 3742, 'gal': 3743, 'backward': 3744, 'honesti': 3745, 'extens': 3746, 'oppress': 3747, 'rant': 3748, 'swallow': 3749, 'phoni': 3750, 'crucial': 3751, 'sue': 3752, 'insur': 3753, 'inject': 3754, 'monologu': 3755, 'robbin': 3756, 'conneri': 3757, 'literatur': 3758, 'poetri': 3759, 'pursuit': 3760, '1968': 3761, 'kingdom': 3762, 'newli': 3763, 'leo': 3764, 'climact': 3765, 'exit': 3766, 'rubi': 3767, 'tackl': 3768, 'countrysid': 3769, 'montana': 3770, 'furi': 3771, 'prey': 3772, 'util': 3773, 'june': 3774, 'drain': 3775, 'tenant': 3776, 'romp': 3777, 'bitch': 3778, 'calm': 3779, 'tourist': 3780, 'prequel': 3781, 'instrument': 3782, 'damm': 3783, 'tempt': 3784, 'stale': 3785, 'rocki': 3786, 'simmon': 3787, 'slash': 3788, 'belushi': 3789, 'alarm': 3790, 'deaf': 3791, 'wisdom': 3792, 'nostalgia': 3793, 'attorney': 3794, 'myth': 3795, 'profess': 3796, 'distinguish': 3797, 'im': 3798, 'orson': 3799, 'bunni': 3800, 'franki': 3801, 'region': 3802, 'cannon': 3803, 'bay': 3804, 'flee': 3805, 'mislead': 3806, 'secretli': 3807, '3d': 3808, 'fed': 3809, 'hum': 3810, 'gambl': 3811, 'austen': 3812, 'outing': 3813, 'dicken': 3814, 'belt': 3815, 'virginia': 3816, 'tunnel': 3817, 'trend': 3818, 'carel': 3819, 'disabl': 3820, 'sirk': 3821, 'duck': 3822, 'twilight': 3823, 'omen': 3824, 'acid': 3825, 'miseri': 3826, 'scottish': 3827, 'valuabl': 3828, 'banal': 3829, 'unpredict': 3830, 'bonu': 3831, 'dazzl': 3832, 'champion': 3833, 'rotten': 3834, 'grotesqu': 3835, 'onlin': 3836, 'heal': 3837, 'wore': 3838, 'rubber': 3839, 'phantom': 3840, 'attribut': 3841, 'campbel': 3842, 'dictat': 3843, 'employe': 3844, 'melodi': 3845, 'bela': 3846, 'lena': 3847, 'profan': 3848, 'louis': 3849, 'meander': 3850, 'ashley': 3851, 'thompson': 3852, 'mortal': 3853, 'damon': 3854, 'rude': 3855, 'econom': 3856, 'giggl': 3857, 'beatl': 3858, 'expand': 3859, 'nephew': 3860, 'agenda': 3861, 'predat': 3862, 'underli': 3863, 'map': 3864, 'mute': 3865, 'elm': 3866, 'wheel': 3867, 'warmth': 3868, 'household': 3869, 'din': 3870, 'regist': 3871, 'interior': 3872, 'underst': 3873, 'dispos': 3874, 'greedi': 3875, 'junior': 3876, 'wacki': 3877, 'vein': 3878, 'volum': 3879, 'wax': 3880, 'closest': 3881, 'randi': 3882, 'duval': 3883, 'pink': 3884, 'travesti': 3885, 'rooney': 3886, 'clone': 3887, 'roth': 3888, 'convincingli': 3889, 'glanc': 3890, 'meg': 3891, 'gestur': 3892, 'emphas': 3893, 'butler': 3894, 'deniro': 3895, 'exposur': 3896, 'ridden': 3897, 'blackmail': 3898, 'nichola': 3899, 'jan': 3900, 'indiffer': 3901, 'fleet': 3902, 'championship': 3903, 'marion': 3904, 'inher': 3905, 'priceless': 3906, 'miyazaki': 3907, 'coast': 3908, 'plod': 3909, 'reign': 3910, 'crystal': 3911, 'dolph': 3912, 'nicol': 3913, 'descent': 3914, 'septemb': 3915, 'occupi': 3916, 'mankind': 3917, 'othello': 3918, 'stack': 3919, 'shove': 3920, 'sabrina': 3921, 'firstli': 3922, 'compliment': 3923, 'represent': 3924, 'mon': 3925, 'chess': 3926, 'habit': 3927, 'disord': 3928, 'dysfunct': 3929, 'firm': 3930, 'heel': 3931, 'chicago': 3932, 'proce': 3933, 'nyc': 3934, 'cleverli': 3935, 'edgar': 3936, 'exclus': 3937, 'scotland': 3938, 'raymond': 3939, 'analysi': 3940, 'peril': 3941, 'worn': 3942, 'roof': 3943, 'millionair': 3944, 'enthusiasm': 3945, 'unleash': 3946, 'orang': 3947, 'daisi': 3948, 'unseen': 3949, 'coup': 3950, 'grayson': 3951, 'bike': 3952, 'perpetu': 3953, 'rukh': 3954, 'yawn': 3955, 'ustinov': 3956, 'fart': 3957, 'dreari': 3958, 'mutant': 3959, 'soderbergh': 3960, 'kolchak': 3961, 'kurosawa': 3962, 'accuraci': 3963, 'judgment': 3964, 'unrel': 3965, 'basketbal': 3966, 'passeng': 3967, 'calib': 3968, 'josh': 3969, 'cent': 3970, 'minimum': 3971, 'surpass': 3972, 'critiqu': 3973, 'preston': 3974, 'showdown': 3975, 'macarthur': 3976, 'femm': 3977, 'wang': 3978, 'businessman': 3979, 'ross': 3980, 'rout': 3981, 'bump': 3982, 'undeni': 3983, 'chronicl': 3984, 'likewis': 3985, 'drake': 3986, 'cream': 3987, 'stuart': 3988, 'sailor': 3989, '1973': 3990, 'bridget': 3991, 'mode': 3992, 'marc': 3993, 'programm': 3994, 'randomli': 3995, 'tube': 3996, 'slam': 3997, 'meryl': 3998, 'franco': 3999, 'tacki': 4000, 'temper': 4001, 'incorpor': 4002, 'spacey': 4003, 'sergeant': 4004, 'peck': 4005, 'reluct': 4006, 'crawl': 4007, 'lengthi': 4008, 'keith': 4009, 'alongsid': 4010, 'hypnot': 4011, 'stroke': 4012, 'recognis': 4013, 'rapist': 4014, 'stake': 4015, 'dee': 4016, 'provoc': 4017, 'valley': 4018, 'loyalti': 4019, 'equival': 4020, 'nathan': 4021, 'verbal': 4022, 'wwe': 4023, 'exposit': 4024, 'what': 4025, 'meyer': 4026, 'irrelev': 4027, 'baldwin': 4028, 'watson': 4029, 'sustain': 4030, '1995': 4031, 'jewel': 4032, 'hay': 4033, 'spoke': 4034, 'compromis': 4035, 'startl': 4036, 'jodi': 4037, 'radic': 4038, 'sand': 4039, 'greed': 4040, 'er': 4041, 'waitress': 4042, 'spain': 4043, 'reson': 4044, 'climat': 4045, 'nineti': 4046, 'fenc': 4047, 'horn': 4048, 'tiresom': 4049, 'greg': 4050, 'buffalo': 4051, 'dana': 4052, 'shoddi': 4053, 'ritual': 4054, 'testament': 4055, 'sid': 4056, 'profit': 4057, 'edgi': 4058, 'nun': 4059, 'expedit': 4060, 'latin': 4061, 'molli': 4062, 'condemn': 4063, 'shaw': 4064, 'degrad': 4065, 'foxx': 4066, 'unorigin': 4067, 'drift': 4068, 'crown': 4069, 'painter': 4070, 'contradict': 4071, 'absent': 4072, 'simplic': 4073, '1933': 4074, 'logan': 4075, 'fog': 4076, 'sammi': 4077, 'approv': 4078, 'russia': 4079, 'uncov': 4080, 'spock': 4081, 'crow': 4082, 'behaviour': 4083, 'passabl': 4084, 'petti': 4085, 'shootout': 4086, 'delic': 4087, 'stargat': 4088, 'han': 4089, 'vet': 4090, 'prank': 4091, 'freez': 4092, 'pocket': 4093, 'bloom': 4094, 'rear': 4095, 'substitut': 4096, 'pamela': 4097, 'ponder': 4098, '13th': 4099, 'correctli': 4100, 'demis': 4101, 'orlean': 4102, 'joel': 4103, 'mighti': 4104, 'wretch': 4105, 'drip': 4106, 'basing': 4107, 'monoton': 4108, 'wagner': 4109, 'stallon': 4110, '19th': 4111, 'barrel': 4112, 'bro': 4113, 'unsatisfi': 4114, 'cooki': 4115, 'alison': 4116, 'unexpectedli': 4117, 'improvis': 4118, 'campaign': 4119, 'derang': 4120, 'murray': 4121, 'empathi': 4122, 'distress': 4123, 'bias': 4124, 'asylum': 4125, 'sucker': 4126, 'cloud': 4127, 'legaci': 4128, 'lundgren': 4129, 'suffici': 4130, 'robber': 4131, 'baddi': 4132, 'darren': 4133, 'unfair': 4134, 'enforc': 4135, 'sentinel': 4136, 'isabel': 4137, 'realm': 4138, 'alec': 4139, 'conduct': 4140, '1987': 4141, 'kansa': 4142, 'buster': 4143, 'counterpart': 4144, 'primit': 4145, 'vanc': 4146, 'novak': 4147, 'unawar': 4148, 'kumar': 4149, '3000': 4150, 'yeti': 4151, 'mormon': 4152, 'outlin': 4153, 'grief': 4154, 'characteris': 4155, 'taxi': 4156, 'brenda': 4157, 'unravel': 4158, 'exquisit': 4159, 'underneath': 4160, 'fright': 4161, 'nolan': 4162, 'antonioni': 4163, 'palac': 4164, 'mtv': 4165, 'gerard': 4166, 'pale': 4167, 'vital': 4168, 'roller': 4169, 'restrain': 4170, 'preserv': 4171, 'rumor': 4172, 'sunni': 4173, 'technicolor': 4174, 'behold': 4175, 'reid': 4176, 'rehears': 4177, 'perceiv': 4178, 'crawford': 4179, 'palanc': 4180, 'censor': 4181, 'wig': 4182, 'shell': 4183, 'dreck': 4184, 'boxer': 4185, 'darn': 4186, 'simultan': 4187, 'seldom': 4188, 'newman': 4189, '2008': 4190, 'penni': 4191, 'bake': 4192, 'anchor': 4193, 'labor': 4194, 'patienc': 4195, 'iv': 4196, 'unimagin': 4197, 'shaki': 4198, 'trait': 4199, 'stark': 4200, 'culmin': 4201, 'pete': 4202, 'hammi': 4203, 'mclaglen': 4204, 'warrant': 4205, 'inmat': 4206, 'improb': 4207, 'abomin': 4208, 'tech': 4209, 'globe': 4210, 'tomorrow': 4211, 'kitti': 4212, 'implic': 4213, 'click': 4214, 'iraq': 4215, 'lush': 4216, 'session': 4217, 'conscious': 4218, 'despis': 4219, 'schedul': 4220, 'feat': 4221, 'lurk': 4222, 'airport': 4223, 'yearn': 4224, 'guitar': 4225, 'angela': 4226, 'analyz': 4227, 'en': 4228, 'runner': 4229, 'austin': 4230, 'gregori': 4231, 'lang': 4232, 'delv': 4233, '1978': 4234, 'glorifi': 4235, 'sniper': 4236, 'spice': 4237, '1971': 4238, 'consciou': 4239, 'weav': 4240, 'agenc': 4241, 'shortcom': 4242, 'aristocrat': 4243, 'drove': 4244, 'liberti': 4245, 'helpless': 4246, 'propos': 4247, 'lure': 4248, 'rhythm': 4249, 'buzz': 4250, 'eaten': 4251, 'polici': 4252, 'puppi': 4253, 'barrymor': 4254, 'mytholog': 4255, 'fundament': 4256, 'rub': 4257, 'springer': 4258, 'visitor': 4259, 'transcend': 4260, 'exhaust': 4261, 'tactic': 4262, 'valid': 4263, 'filler': 4264, 'dont': 4265, 'conscienc': 4266, 'stalker': 4267, 'thunderbird': 4268, 'secondari': 4269, 'gentleman': 4270, 'el': 4271, 'fuller': 4272, 'furiou': 4273, 'hepburn': 4274, 'lindsay': 4275, 'clerk': 4276, 'wrench': 4277, '22': 4278, 'smell': 4279, 'distort': 4280, 'errol': 4281, 'geek': 4282, 'sublim': 4283, 'brood': 4284, 'clau': 4285, 'mccoy': 4286, 'facil': 4287, 'lampoon': 4288, '1920': 4289, 'aborigin': 4290, 'literari': 4291, 'coaster': 4292, 'dirt': 4293, 'bronson': 4294, 'stream': 4295, 'chainsaw': 4296, 'abrupt': 4297, 'passag': 4298, 'sweep': 4299, 'suspicion': 4300, 'ladder': 4301, 'hallucin': 4302, '1997': 4303, 'underworld': 4304, 'tendenc': 4305, 'tarantino': 4306, 'rehash': 4307, 'laurenc': 4308, 'geni': 4309, 'convert': 4310, 'karl': 4311, 'fought': 4312, '1989': 4313, 'bacal': 4314, 'flair': 4315, 'invad': 4316, 'inabl': 4317, 'financ': 4318, 'cush': 4319, 'tasteless': 4320, 'chamberlain': 4321, 'exterior': 4322, 'crippl': 4323, 'atroc': 4324, 'couch': 4325, 'valentin': 4326, 'straightforward': 4327, 'martha': 4328, 'curli': 4329, 'preposter': 4330, 'attenborough': 4331, 'canyon': 4332, 'sweat': 4333, 'sheet': 4334, 'connor': 4335, '1939': 4336, 'eastern': 4337, 'narrow': 4338, 'oblig': 4339, 'out': 4340, 'graini': 4341, 'vomit': 4342, 'esther': 4343, 'quietli': 4344, 'quinn': 4345, 'tax': 4346, 'kidman': 4347, 'antwon': 4348, 'clash': 4349, 'teas': 4350, 'cyborg': 4351, 'hain': 4352, 'pretens': 4353, 'entitl': 4354, 'rampag': 4355, 'vader': 4356, 'tripe': 4357, '1979': 4358, 'acquaint': 4359, 'amazon': 4360, 'paramount': 4361, 'heartfelt': 4362, 'spontan': 4363, 'ace': 4364, 'wholli': 4365, 'dylan': 4366, 'clueless': 4367, 'israel': 4368, 'scoop': 4369, 'seal': 4370, 'delet': 4371, 'weather': 4372, 'pole': 4373, 'cigarett': 4374, 'bernard': 4375, 'fabric': 4376, 'deceas': 4377, '75': 4378, 'standout': 4379, 'newcom': 4380, 'shanghai': 4381, '1936': 4382, 'tierney': 4383, 'moder': 4384, 'wildli': 4385, 'squad': 4386, 'circu': 4387, 'athlet': 4388, 'net': 4389, 'naughti': 4390, 'missil': 4391, 'scriptwrit': 4392, 'claud': 4393, 'shred': 4394, 'enabl': 4395, 'restrict': 4396, 'cycl': 4397, 'heist': 4398, 'rosemari': 4399, 'divid': 4400, 'policeman': 4401, 'cohen': 4402, 'sugar': 4403, 'hungri': 4404, 'aesthet': 4405, 'parson': 4406, 'recogniz': 4407, 'bounc': 4408, 'minu': 4409, '1988': 4410, 'steer': 4411, 'rita': 4412, 'choppi': 4413, 'loath': 4414, 'luka': 4415, 'immers': 4416, 'gere': 4417, 'scar': 4418, 'obstacl': 4419, 'chavez': 4420, 'hostag': 4421, 'bean': 4422, 'trivia': 4423, 'cap': 4424, 'malon': 4425, 'harmless': 4426, 'hackney': 4427, 'bacon': 4428, 'filth': 4429, 'mathieu': 4430, 'immatur': 4431, 'spiral': 4432, 'sooner': 4433, 'miracul': 4434, 'impos': 4435, 'keen': 4436, 'mutual': 4437, 'flirt': 4438, 'phil': 4439, 'fishburn': 4440, 'shatter': 4441, 'pixar': 4442, 'skeptic': 4443, 'moreov': 4444, 'paradis': 4445, 'enthral': 4446, 'zane': 4447, 'eugen': 4448, '73': 4449, 'gasp': 4450, 'hokey': 4451, 'intric': 4452, 'ram': 4453, 'contempt': 4454, 'nemesi': 4455, 'alicia': 4456, 'debt': 4457, 'posey': 4458, 'duel': 4459, '1993': 4460, 'pickford': 4461, 'beverli': 4462, 'sleepwalk': 4463, 'soup': 4464, 'foolish': 4465, 'ingeni': 4466, 'timberlak': 4467, 'lacklust': 4468, 'insipid': 4469, 'tripl': 4470, 'spree': 4471, 'norm': 4472, 'injuri': 4473, 'enlist': 4474, 'rhyme': 4475, 'uh': 4476, 'baffl': 4477, 'milo': 4478, '1969': 4479, 'sharon': 4480, 'oppon': 4481, '19': 4482, 'messi': 4483, 'rod': 4484, 'leigh': 4485, 'disastr': 4486, 'headach': 4487, 'outlaw': 4488, 'ratso': 4489, 'www': 4490, 'frontier': 4491, 'suprem': 4492, 'strive': 4493, 'bewar': 4494, 'perman': 4495, 'kline': 4496, 'strongest': 4497, 'viewpoint': 4498, 'hk': 4499, 'shepherd': 4500, 'grudg': 4501, 'down': 4502, 'unexplain': 4503, 'transplant': 4504, 'entranc': 4505, 'someday': 4506, 'clan': 4507, 'whore': 4508, 'antholog': 4509, 'grin': 4510, 'mayhem': 4511, 'dwarf': 4512, 'spinal': 4513, 'fuel': 4514, 'dandi': 4515, 'assert': 4516, 'instruct': 4517, 'quaid': 4518, 'egg': 4519, 'mol': 4520, 'lester': 4521, 'watcher': 4522, 'walsh': 4523, 'injur': 4524, 'alexandr': 4525, 'jacket': 4526, 'hopelessli': 4527, 'penn': 4528, 'heap': 4529, 'alley': 4530, 'marlon': 4531, 'traumat': 4532, 'angst': 4533, 'boyl': 4534, 'decept': 4535, 'ish': 4536, 'steadi': 4537, 'astound': 4538, 'nicola': 4539, 'overlong': 4540, 'tick': 4541, 'tyler': 4542, 'sassi': 4543, 'archiv': 4544, 'artsi': 4545, 'undertak': 4546, 'gunga': 4547, 'dame': 4548, 'boston': 4549, 'remad': 4550, 'renaiss': 4551, 'linear': 4552, 'hug': 4553, 'brush': 4554, 'breakfast': 4555, 'flashi': 4556, 'cruelti': 4557, 'reliabl': 4558, 'knightley': 4559, 'info': 4560, 'elit': 4561, 'harold': 4562, 'frantic': 4563, 'preming': 4564, 'patricia': 4565, 'stair': 4566, 'awhil': 4567, 'stimul': 4568, 'cher': 4569, 'razor': 4570, 'scandal': 4571, 'hopkin': 4572, 'choke': 4573, 'setup': 4574, 'helm': 4575, 'barn': 4576, 'antagonist': 4577, '1984': 4578, 'diari': 4579, 'surgeri': 4580, 'camcord': 4581, 'fontain': 4582, 'joker': 4583, 'carla': 4584, 'effici': 4585, 'brit': 4586, 'neurot': 4587, 'variat': 4588, 'audrey': 4589, 'minist': 4590, 'persuad': 4591, 'foil': 4592, '1981': 4593, 'kent': 4594, 'phenomenon': 4595, 'oldest': 4596, 'trigger': 4597, 'spade': 4598, 'wardrob': 4599, 'http': 4600, 'margin': 4601, 'naschi': 4602, 'dish': 4603, 'lucil': 4604, 'scorses': 4605, 'gillian': 4606, 'hilar': 4607, 'off': 4608, 'clad': 4609, 'loretta': 4610, '1976': 4611, 'cassavet': 4612, 'redund': 4613, 'mobil': 4614, 'corman': 4615, 'illus': 4616, 'corn': 4617, 'bitten': 4618, 'fluff': 4619, 'repris': 4620, 'proport': 4621, 'vanessa': 4622, 'clinic': 4623, 'holocaust': 4624, 'schlock': 4625, 'christin': 4626, 'goof': 4627, 'triangl': 4628, 'dim': 4629, 'housewif': 4630, 'taboo': 4631, 'vibrant': 4632, 'nina': 4633, 'poe': 4634, 'youngest': 4635, 'goldsworthi': 4636, 'traffic': 4637, 'nope': 4638, 'banter': 4639, 'wrestler': 4640, 'undermin': 4641, 'morbid': 4642, 'deer': 4643, 'slimi': 4644, 'jade': 4645, 'idol': 4646, 'claustrophob': 4647, 'candl': 4648, 'miniseri': 4649, 'sung': 4650, 'tack': 4651, 'ebert': 4652, 'ingrid': 4653, 'cb': 4654, 'preachi': 4655, 'abund': 4656, 'fascist': 4657, 'mount': 4658, 'bate': 4659, 'spray': 4660, '95': 4661, 'northern': 4662, 'misguid': 4663, 'scarfac': 4664, 'raj': 4665, 'neill': 4666, 'senior': 4667, 'orphan': 4668, 'evelyn': 4669, 'gina': 4670, 'amor': 4671, 'mum': 4672, 'despic': 4673, 'poker': 4674, 'akin': 4675, 'heartwarm': 4676, 'firmli': 4677, 'dudley': 4678, 'dilemma': 4679, 'unattract': 4680, 'downey': 4681, 'winchest': 4682, 'toronto': 4683, 'muscl': 4684, '1986': 4685, 'widescreen': 4686, 'bye': 4687, 'prophet': 4688, 'carlo': 4689, 'relentless': 4690, 'uniformli': 4691, 'sicken': 4692, 'bikini': 4693, 'kazan': 4694, 'nolt': 4695, 'whack': 4696, 'virtu': 4697, 'static': 4698, 'jordan': 4699, 'phenomen': 4700, 'jule': 4701, 'obligatori': 4702, 'durat': 4703, 'rome': 4704, 'bogu': 4705, 'cassidi': 4706, 'bread': 4707, 'gypo': 4708, 'comprehend': 4709, 'hooker': 4710, 'zoom': 4711, 'misfortun': 4712, 'lifeless': 4713, 'cecil': 4714, 'yokai': 4715, 'cancer': 4716, 'alvin': 4717, 'genet': 4718, 'deem': 4719, 'belli': 4720, 'legitim': 4721, 'frog': 4722, 'snatch': 4723, 'departur': 4724, 'carey': 4725, 'redneck': 4726, 'bulk': 4727, 'monument': 4728, 'evolut': 4729, 'newer': 4730, 'worship': 4731, 'jo': 4732, 'burk': 4733, 'preced': 4734, 'vaniti': 4735, 'venom': 4736, 'mitchum': 4737, 'weari': 4738, 'incarn': 4739, 'aris': 4740, 'cow': 4741, 'rot': 4742, 'retriev': 4743, 'pioneer': 4744, 'niec': 4745, 'sleaz': 4746, 'cypher': 4747, 'healthi': 4748, 'fifth': 4749, 'id': 4750, 'shield': 4751, 'casino': 4752, 'flock': 4753, 'blur': 4754, 'discern': 4755, '1994': 4756, 'loneli': 4757, 'mobster': 4758, 'proclaim': 4759, 'brendan': 4760, 'btw': 4761, 'bent': 4762, 'constitut': 4763, 'victorian': 4764, 'creek': 4765, 'astronaut': 4766, 'eli': 4767, 'articl': 4768, 'ariel': 4769, 'loi': 4770, 'bastard': 4771, 'nightclub': 4772, 'smack': 4773, 'dodg': 4774, 'gabl': 4775, 'recit': 4776, 'masterson': 4777, 'energet': 4778, 'cartoonish': 4779, 'summar': 4780, 'hapless': 4781, 'subway': 4782, 'roar': 4783, 'grass': 4784, 'axe': 4785, 'sidewalk': 4786, 'dubiou': 4787, 'peer': 4788, 'earnest': 4789, 'paxton': 4790, 'clara': 4791, 'undead': 4792, 'walt': 4793, 'divin': 4794, 'feast': 4795, 'biker': 4796, 'glare': 4797, 'charlton': 4798, 'turd': 4799, 'artwork': 4800, 'rapidli': 4801, 'affleck': 4802, 'disregard': 4803, 'gilliam': 4804, 'resum': 4805, 'token': 4806, 'armstrong': 4807, 'modest': 4808, 'dismal': 4809, 'alleg': 4810, 'appl': 4811, 'jedi': 4812, 'inflict': 4813, 'magician': 4814, 'macho': 4815, 'spectacl': 4816, 'moe': 4817, 'lighter': 4818, 'civilian': 4819, 'assort': 4820, 'galaxi': 4821, 'psych': 4822, 'sparkl': 4823, 'shelter': 4824, 'submit': 4825, 'mermaid': 4826, 'inaccuraci': 4827, 'cape': 4828, 'hybrid': 4829, 'kinnear': 4830, 'salt': 4831, 'bondag': 4832, 'crosbi': 4833, 'anton': 4834, 'howl': 4835, 'wtf': 4836, 'salman': 4837, 'thiev': 4838, 'braveheart': 4839, 'harrison': 4840, 'wong': 4841, 'seedi': 4842, 'rebelli': 4843, 'jealousi': 4844, 'greet': 4845, 'jam': 4846, 'biopic': 4847, 'motorcycl': 4848, 'shorter': 4849, 'abound': 4850, 'hostil': 4851, 'timmi': 4852, 'comprehens': 4853, 'flavor': 4854, 'mama': 4855, '28': 4856, 'bravo': 4857, 'electron': 4858, 'collector': 4859, 'boob': 4860, 'clutter': 4861, 'pacif': 4862, 'pepper': 4863, 'propheci': 4864, 'congratul': 4865, 'rosario': 4866, 'netflix': 4867, '1991': 4868, 'contend': 4869, 'rooki': 4870, 'polli': 4871, 'corbett': 4872, 'bsg': 4873, 'growth': 4874, 'spawn': 4875, 'unsuspect': 4876, 'ceremoni': 4877, 'mason': 4878, 'estrang': 4879, 'highway': 4880, 'gamera': 4881, 'jill': 4882, 'playwright': 4883, 'landmark': 4884, 'sorrow': 4885, 'harlow': 4886, 'cerebr': 4887, 'salvat': 4888, 'biblic': 4889, 'arrow': 4890, 'truman': 4891, 'orchestr': 4892, 'vain': 4893, 'anyhow': 4894, 'isra': 4895, 'monti': 4896, 'novelti': 4897, 'ol': 4898, 'stunningli': 4899, 'hindi': 4900, 'gypsi': 4901, 'interestingli': 4902, 'liu': 4903, 'curtain': 4904, 'randolph': 4905, 'fluid': 4906, 'horni': 4907, 'uncut': 4908, 'meteor': 4909, '4th': 4910, 'vastli': 4911, 'blatantli': 4912, 'snuff': 4913, 'kathryn': 4914, 'lavish': 4915, 'chip': 4916, 'entireti': 4917, 'faint': 4918, 'neatli': 4919, 'inaccur': 4920, 'scariest': 4921, 'paula': 4922, 'breakdown': 4923, 'spine': 4924, 'lui': 4925, 'colin': 4926, 'ash': 4927, 'occup': 4928, 'bachelor': 4929, 'outright': 4930, 'leather': 4931, 'saddl': 4932, 'historian': 4933, 'knee': 4934, 'gentlemen': 4935, 'conrad': 4936, 'swept': 4937, 'lanc': 4938, 'comb': 4939, 'mutil': 4940, 'lectur': 4941, 'stephani': 4942, 'wendigo': 4943, 'toe': 4944, 'turtl': 4945, 'thrust': 4946, 'cattl': 4947, 'insect': 4948, 'vignett': 4949, 'imo': 4950, 'resent': 4951, 'odyssey': 4952, 'dwell': 4953, 'christina': 4954, 'plug': 4955, 'playboy': 4956, 'cheadl': 4957, 'pumbaa': 4958, 'verg': 4959, 'sgt': 4960, 'subtli': 4961, 'porter': 4962, 'senat': 4963, 'adjust': 4964, 'iren': 4965, 'corridor': 4966, 'spill': 4967, 'lex': 4968, 'fragil': 4969, 'bront': 4970, 'esquir': 4971, 'wonderland': 4972, 'sacrif': 4973, 'ronald': 4974, 'stardom': 4975, 'replay': 4976, 'grandma': 4977, 'caron': 4978, 'melissa': 4979, 'conan': 4980, 'pedestrian': 4981, 'quantum': 4982, 'miami': 4983, 'beard': 4984, 'fruit': 4985, 'cuban': 4986, 'conquer': 4987, 'goldblum': 4988, '1945': 4989, 'huston': 4990, 'caretak': 4991, 'evan': 4992, 'ideolog': 4993, 'tokyo': 4994, 'cal': 4995, 'sensual': 4996, 'detach': 4997, 'ritchi': 4998, 'signal': 4999}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4998"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = build_dict(train_set_processed)\n",
    "print(word_dict)\n",
    "len(word_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9916e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "\n",
    "    working_sentence = [NOWORD] * pad\n",
    "\n",
    "    #  We iterate through the words in the input sentence. We also slice the sentence up to the pad length to handle reviews longer than our desired sequence length\n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "\n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "\n",
    "    # We iterate through each sentence in the input data.\n",
    "    for sentence in data:\n",
    "        # For each sentence, we call the convert_and_pad function to get the processed integer sequence (converted) and its original length (leng).\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "\n",
    "        # We append the converted sequence to our result list and the leng to our lengths list.\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "\n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6437a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_tokenized, train_set_tokenized_len = convert_and_pad_data(word_dict, train_set_processed)\n",
    "val_set_tokenized, val_set_tokenized_len = convert_and_pad_data(word_dict, val_set_processed)\n",
    "test_set_tokenized, test_set_tokenized_len = convert_and_pad_data(word_dict, test_set_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad27e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 45   2 136 ...   0   0   0]\n",
      " [244 122 408 ...   0   0   0]\n",
      " [  5   1 338 ...   0   0   0]\n",
      " [ 12   2   4 ...   0   0   0]\n",
      " [120 960 139 ...   0   0   0]]\n",
      "[[4321  120 1016 ...    0    0    0]\n",
      " [  60   93   44 ...    0    0    0]\n",
      " [ 400  901  230 ...    0    0    0]\n",
      " [3129  185  575 ...    0    0    0]\n",
      " [   1  505   85 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_set_tokenized[0:5])\n",
    "print(val_set_tokenized[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16db29df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.', 'Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV\\'s \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina\\'s pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles & Beverly D\\'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"Rosemary\\'s Baby\" and \"The Exorcist\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"The Sentinel\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****', 'A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation while they waited for Robbins to retrieve the birdie.', 'It\\'s a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.<br /><br />Based upon Thomas Rockwell\\'s respected Book, How To Eat Fried Worms starts like any children\\'s story: moving to a new town. The new kid, fifth grader Billy Forrester was once popular, but has to start anew. Making friends is never easy, especially when the only prospect is Poindexter Adam. Or Erica, who at 4 1/2 feet, is a giant.<br /><br />Further complicating things is Joe the bully. His freckled face and sleeveless shirts are daunting. He antagonizes kids with the Death Ring: a Crackerjack ring that is rumored to kill you if you\\'re punched with it. But not immediately. No, the death ring unleashes a poison that kills you in the eight grade.<br /><br />Joe and his axis of evil welcome Billy by smuggling a handful of slimy worms into his thermos. Once discovered, Billy plays it cool, swearing that he eats worms all the time. Then he throws them at Joe\\'s face. Ewww! To win them over, Billy reluctantly bets that he can eat 10 worms. Fried, boiled, marinated in hot sauce, squashed and spread on a peanut butter sandwich. Each meal is dubbed an exotic name like the \"Radioactive Slime Delight,\" in which the kids finally live out their dream of microwaving a living organism.<br /><br />If you\\'ve ever met me, you\\'ll know that I have an uncontrollably hearty laugh. I felt like a creep erupting at a toddler whining that his \"dilly dick\" hurts. But Fried Worms is wonderfully disgusting. Like a G-rated Farrelly brothers film, it is both vomitous and delightful.<br /><br />Writer/director Bob Dolman is also a savvy storyteller. To raise the stakes the worms must be consumed by 7 pm. In addition Billy holds a dark secret: he has an ultra-sensitive stomach.<br /><br />Dolman also has a keen sense of perspective. With such accuracy, he draws on children\\'s insecurities and tendency to exaggerate mundane dilemmas.<br /><br />If you were to hyperbolize this movie the way kids do their quandaries, you will see that it is essentially about war. Freedom-fighter and freedom-hater use pubescent boys as pawns in proxy wars, only to learn a valuable lesson in unity. International leaders can learn a thing or two about global peacekeeping from Fried Worms.<br /><br />At the end of the film, I was comforted when two chaperoning mothers behind me, looked at each other with befuddlement and agreed, \"That was a great movie.\" Great, now I won\\'t have to register myself in any lawful databases.', \"You probably all already know this by now, but 5 additional episodes never aired can be viewed on ABC.com I've watched a lot of television over the years and this is possibly my favorite show, ever. It's a crime that this beautifully written and acted show was canceled. The actors that played Laura, Whit, Carlos, Mae, Damian, Anya and omg, Steven Caseman - are all incredible and so natural in those roles. Even the kids are great. Wonderful show. So sad that it's gone. Of course I wonder about the reasons it was canceled. There is no way I'll let myself believe that Ms. Moynahan's pregnancy had anything to do with it. It was in the perfect time slot in this market. I've watched all the episodes again on ABC.com - I hope they all come out on DVD some day. Thanks for reading.\"]\n",
      "[1, 1, 1, 1, 1]\n",
      "\n",
      "[\"I have NOT seen this movie, but I must. Having read all three of Thor Heyerdahl's books (Kon Tiki, Ra and Aku Aku) I am actively looking for a copy of this movie.<br /><br />The thesis that Peruvians migrated to Polynesia is alive and well. Considering that this crew had NO GPS, and only an old fashioned valve (tube) radio with a 6-watt output, their voyage was heroic to say the least.<br /><br />Please reply to this message if you can tell me the location of a copy of this video.<br /><br />I would be interested in buying it.\", \"I can't understand what it is that fans of the genre didn't like about this film. It was truly a lot of fun. The special effects were wonderful. I generally agree with reviews and with IMDB voters, but not this time. I waited until it came to home video which I felt was another reason that I wouldn't enjoy the film. I believe special effects films need to be seen on the big screen, but again this was not the case. To me the film begs comparison to two films that were released around the same time. Blair Witch and The Mummy. Both films that I thought were terrible. Blair was probably the most overrated horror film of all time. The Mummy made gobs of money and it was pure dreck. People liked it for it's special effects. Films like the Mummy and The Haunting are not rich in character development, they are more like funhouse rides. Well with that analogy the Mummy was a B ticket to the Haunting's E-ticket.\", 'Like his elder brothers, Claude Sautet and Jean-Pierre Melville, Alain Corneau began to cut his teeth in French cinema with a series of fine thrillers: \"la Menace\" (1977) and \"Série Noire\" (1979) among others. \"Police Python 357\" is a good example of how Corneau conceived and shot his works at this time of his career. They had a splendid cinematography, painstaking screenplays and a sophisticated directing elaborated for efficiency\\'s sake.<br /><br />The police superintendent Ferrot (Yves Montand) is a cop with unconventional methods who usually works all alone. He makes the acquaintance of a young woman Sylvia Léopardi (Stefania Sandrelli) and becomes her lover while ignoring that she has another lover: his superior Ganay (François Périer). When the latter learns it, he kills her in a fit of anger. Ferrot has to investigate the murder and all the clues are inexorably against him...<br /><br />One could deem that this kind of far-fetched story isn\\'t exempt from glitches and sometimes, one can see right through it but Corneau\\'s pedantic directorial style helps to conjure up a stifling, dusky atmosphere. The first part of the film before the night of the murder might seem uninteresting and however, it is crucial for what will follow this key-moment. Corneau falls back on a sober treatment with rather sparse moments and short appearances by secondary, minor characters whom the viewer will see again during the investigation. In spite of drawbacks, Corneau and his scenarist Daniel Boulanger penned a deft story. Ménard (Mathieu Carrière) who sometimes expresses his surprise because Ferrot keeps a relatively low profile during the investigation. But his superior knows that he usually works alone. Actually, Ferrot has to find solid tricks to muddy the waters and so to exonerate himself. Eventually, the chief idea of the film concerns Ferrot himself. He\\'s a cop who bit by bit loses his identity and finds himself in the heart of a terrible depersonalization. It is epitomized by the moment when he throws himself acid on his face so that witnesses won\\'t recognize him when he is brought face to face with them.<br /><br />The backdrop of this thriller, Orléans is efficiently enhanced by Corneau\\'s camera and helps to inspire this eerie thriller its pernicious charm.', \"I was watching this movie at one of my usual time, which is real real late at night. Usually if a movie doesn't interest me, I start falling asleep and have to raid the fridge to stay awake.<br /><br />At first I thought that's what I had to do since this movie's pacing started off slow, along with the fact that its shots tended to linger with the character for a long time. But after a bit, I start getting more into the movie, as more is revealed about the main character through his story telling. By the end, you feel like you've known him your whole life. The movie kept my interest so much that I didn't even know the sun was about to rise.<br /><br />Not much of Lynch's bizzare style, but there is enough of quirky characters to make the film amusing.\", \"We, as a family, were so delighted with 'The Last of the Blonde Bombshells' we purchased a copy for our home video library.<br /><br />The acting is A1 and the cast contains many favorite actors and singers. The theme is unusual and the script well written. The music/songs are timeless and takes us back to our young days when we sang the songs at the top of our voices. To outline the story here would spoil the 'plot' as it is really nice to sit back and enjoy the story as it unfolds.<br /><br />Full marks to this most enjoyable and uplifting production and we heartily recommend it to anyone who is looking for a belly-laugh and lots of music.\"]\n",
      "[1, 1, 1, 1, 1]\n",
      "\n",
      "[['seen', 'movi', 'must', 'read', 'three', 'thor', 'heyerdahl', 'book', 'kon', 'tiki', 'ra', 'aku', 'aku', 'activ', 'look', 'copi', 'movi', 'thesi', 'peruvian', 'migrat', 'polynesia', 'aliv', 'well', 'consid', 'crew', 'gp', 'old', 'fashion', 'valv', 'tube', 'radio', '6', 'watt', 'output', 'voyag', 'heroic', 'say', 'least', 'pleas', 'repli', 'messag', 'tell', 'locat', 'copi', 'video', 'would', 'interest', 'buy'], ['understand', 'fan', 'genr', 'like', 'film', 'truli', 'lot', 'fun', 'special', 'effect', 'wonder', 'gener', 'agre', 'review', 'imdb', 'voter', 'time', 'wait', 'came', 'home', 'video', 'felt', 'anoth', 'reason', 'enjoy', 'film', 'believ', 'special', 'effect', 'film', 'need', 'seen', 'big', 'screen', 'case', 'film', 'beg', 'comparison', 'two', 'film', 'releas', 'around', 'time', 'blair', 'witch', 'mummi', 'film', 'thought', 'terribl', 'blair', 'probabl', 'overr', 'horror', 'film', 'time', 'mummi', 'made', 'gob', 'money', 'pure', 'dreck', 'peopl', 'like', 'special', 'effect', 'film', 'like', 'mummi', 'haunt', 'rich', 'charact', 'develop', 'like', 'funhous', 'ride', 'well', 'analog', 'mummi', 'b', 'ticket', 'haunt', 'e', 'ticket'], ['like', 'elder', 'brother', 'claud', 'sautet', 'jean', 'pierr', 'melvil', 'alain', 'corneau', 'began', 'cut', 'teeth', 'french', 'cinema', 'seri', 'fine', 'thriller', 'la', 'menac', '1977', 'rie', 'noir', '1979', 'among', 'other', 'polic', 'python', '357', 'good', 'exampl', 'corneau', 'conceiv', 'shot', 'work', 'time', 'career', 'splendid', 'cinematographi', 'painstak', 'screenplay', 'sophist', 'direct', 'elabor', 'effici', 'sake', 'polic', 'superintend', 'ferrot', 'yve', 'montand', 'cop', 'unconvent', 'method', 'usual', 'work', 'alon', 'make', 'acquaint', 'young', 'woman', 'sylvia', 'l', 'opardi', 'stefania', 'sandrelli', 'becom', 'lover', 'ignor', 'anoth', 'lover', 'superior', 'ganay', 'fran', 'oi', 'p', 'rier', 'latter', 'learn', 'kill', 'fit', 'anger', 'ferrot', 'investig', 'murder', 'clue', 'inexor', 'one', 'could', 'deem', 'kind', 'far', 'fetch', 'stori', 'exempt', 'glitch', 'sometim', 'one', 'see', 'right', 'corneau', 'pedant', 'directori', 'style', 'help', 'conjur', 'stifl', 'duski', 'atmospher', 'first', 'part', 'film', 'night', 'murder', 'might', 'seem', 'uninterest', 'howev', 'crucial', 'follow', 'key', 'moment', 'corneau', 'fall', 'back', 'sober', 'treatment', 'rather', 'spars', 'moment', 'short', 'appear', 'secondari', 'minor', 'charact', 'viewer', 'see', 'investig', 'spite', 'drawback', 'corneau', 'scenarist', 'daniel', 'boulang', 'pen', 'deft', 'stori', 'nard', 'mathieu', 'carri', 'sometim', 'express', 'surpris', 'ferrot', 'keep', 'rel', 'low', 'profil', 'investig', 'superior', 'know', 'usual', 'work', 'alon', 'actual', 'ferrot', 'find', 'solid', 'trick', 'muddi', 'water', 'exoner', 'eventu', 'chief', 'idea', 'film', 'concern', 'ferrot', 'cop', 'bit', 'bit', 'lose', 'ident', 'find', 'heart', 'terribl', 'deperson', 'epitom', 'moment', 'throw', 'acid', 'face', 'wit', 'recogn', 'brought', 'face', 'face', 'backdrop', 'thriller', 'orl', 'an', 'effici', 'enhanc', 'corneau', 'camera', 'help', 'inspir', 'eeri', 'thriller', 'pernici', 'charm'], ['watch', 'movi', 'one', 'usual', 'time', 'real', 'real', 'late', 'night', 'usual', 'movi', 'interest', 'start', 'fall', 'asleep', 'raid', 'fridg', 'stay', 'awak', 'first', 'thought', 'sinc', 'movi', 'pace', 'start', 'slow', 'along', 'fact', 'shot', 'tend', 'linger', 'charact', 'long', 'time', 'bit', 'start', 'get', 'movi', 'reveal', 'main', 'charact', 'stori', 'tell', 'end', 'feel', 'like', 'known', 'whole', 'life', 'movi', 'kept', 'interest', 'much', 'even', 'know', 'sun', 'rise', 'much', 'lynch', 'bizzar', 'style', 'enough', 'quirki', 'charact', 'make', 'film', 'amus'], ['famili', 'delight', 'last', 'blond', 'bombshel', 'purchas', 'copi', 'home', 'video', 'librari', 'act', 'a1', 'cast', 'contain', 'mani', 'favorit', 'actor', 'singer', 'theme', 'unusu', 'script', 'well', 'written', 'music', 'song', 'timeless', 'take', 'us', 'back', 'young', 'day', 'sang', 'song', 'top', 'voic', 'outlin', 'stori', 'would', 'spoil', 'plot', 'realli', 'nice', 'sit', 'back', 'enjoy', 'stori', 'unfold', 'full', 'mark', 'enjoy', 'uplift', 'product', 'heartili', 'recommend', 'anyon', 'look', 'belli', 'laugh', 'lot', 'music']]\n",
      "[1, 1, 1, 1, 1]\n",
      "\n",
      "[[ 45   2 136 ...   0   0   0]\n",
      " [244 122 408 ...   0   0   0]\n",
      " [  5   1 338 ...   0   0   0]\n",
      " [ 12   2   4 ...   0   0   0]\n",
      " [120 960 139 ...   0   0   0]]\n",
      "[1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['pos'][0:5])\n",
    "print(labels['train']['pos'][0:5])\n",
    "print()\n",
    "\n",
    "print(train_set[0:5])\n",
    "print(train_labels[0:5])\n",
    "print()\n",
    "\n",
    "print(train_set_processed[0:5])\n",
    "print(train_labels[0:5])\n",
    "print()\n",
    "\n",
    "print(train_set_tokenized[0:5])\n",
    "print(train_labels[0:5])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00410a",
   "metadata": {},
   "source": [
    "### Implementing the LSTM text classifier\n",
    "\n",
    "While coding the LSTM text classifier, I learned that there are some additional settings for the embedding\\\n",
    "layer. `padding_idx` will make the LSTM model ignore \"0\" - which is the padding we used when preprocessing\\\n",
    "data. Also, during the forward step, we will have to grab each batch's last non zero value. Otherwise if \\\n",
    "those steps are not done then the \"0\" padding will update the model's state and mess up the output.\\\n",
    "\\\n",
    "Additionally, the padding makes all lines end in the same pattern, which also causes bias in the model's\\\n",
    "training. \\\n",
    "\\\n",
    "I will experiment with different factors that affect the LSTM, including:\n",
    "- Learning Rate\n",
    "- Batch Size\n",
    "- Number of epochs\n",
    "- Hidden size\n",
    "- Number of layers\n",
    "- Optimization Algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7cd22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "                super().__init__()\n",
    "\n",
    "                # This creates the embedding layer. It takes integer indices (representing words) as input and maps them to dense vectors of size (embedding_dim)\n",
    "                self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "                 # This defines the LSTM layer.\n",
    "                self.lstm = nn.LSTM(embedding_dim, # The input size to the LSTM (the dimensionality of the embeddings).\n",
    "                                    hidden_dim, # The number of hidden units in the LSTM\n",
    "                                    num_layers= n_layers, # This specifies the number of LSTM layers to stack.\n",
    "                                    batch_first=True # This is a crucial parameter. It tells the LSTM that the input and output tensors will have the batch size as the first dimension (i.e., [batch_size, seq_length, feature_dim]). This is a common convention in PyTorch.    \n",
    "                                    )\n",
    "                # This creates a fully connected (linear) layer that will take the output from the LSTM and produce a single output value for each review.\n",
    "                # The input size is hidden_dim (the dimensionality of the final hidden state we'll use), and the output size is 1 (for binary classification).\n",
    "                self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "                emb, _ = self.embedding(x), None\n",
    "                out, _ = self.lstm(emb)                                   # [B, L, H]\n",
    "                lengths = (x != 0).sum(dim=1)                             # [B]\n",
    "                # grab each batch’s last non-pad timestep\n",
    "                last_hid = out[torch.arange(out.size(0)), lengths-1]      # [B, H]\n",
    "                return self.fc(last_hid)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e93110a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_model(epochs, model, train_loader, device, optimizer, loss_fn, val_loader):\n",
    "        # The following code is similar to what we implement in Project Assignment 1\n",
    "        for epoch in range(1, epochs + 1):\n",
    "                model.train()\n",
    "                total_loss = 0.0\n",
    "                count = 0\n",
    "                running_loss = 0.0\n",
    "                for batch in train_loader:\n",
    "                        batch_X, batch_y = batch\n",
    "\n",
    "                        batch_X = batch_X.to(device)\n",
    "                        batch_y = batch_y.to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model.forward(batch_X).squeeze(1)\n",
    "\n",
    "                        loss = loss_fn(output, batch_y)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        total_loss += loss.data.item()\n",
    "                        running_loss += loss.data.item()\n",
    "\n",
    "                        count+=1\n",
    "                        if count%100 == 0:\n",
    "                                print(\"Epoch: {}, BCELoss: {}\".format(epoch, running_loss/100.0))\n",
    "                                running_loss = 0.0\n",
    "\n",
    "                avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "                #---validation---\n",
    "                model.eval()\n",
    "                total_val_loss = 0.0\n",
    "                total_correct = 0\n",
    "                total_samples = 0\n",
    "                with torch.no_grad():\n",
    "                        for batch in val_loader:\n",
    "                                batch_X, batch_y = batch\n",
    "                                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                                output = model(batch_X).squeeze(1)\n",
    "                                total_val_loss += loss_fn(output, batch_y).item()\n",
    "\n",
    "                                preds = (torch.sigmoid(output) >= 0.5).float()\n",
    "                                total_correct += (preds == batch_y).sum().item()\n",
    "                                total_samples  += batch_y.size(0)\n",
    "                avg_val_loss = total_val_loss / len(val_loader)\n",
    "                val_acc     = total_correct / total_samples\n",
    "                # ——— LOGGING ———\n",
    "                print(f\"Epoch {epoch}/{epochs} — \"\n",
    "                f\"Avg Train Loss: {avg_train_loss:.4f} | \"\n",
    "                f\"Val Loss:   {avg_val_loss:.4f} | \"\n",
    "                f\"Val Acc:    {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba25c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_tensors(train_set_tokenized, train_labels):\n",
    "        train_data_tensor = torch.tensor(train_set_tokenized, dtype=torch.long)\n",
    "        train_label_tensor = torch.tensor(train_labels, dtype=torch.float)  # or torch.long for class indices\n",
    "\n",
    "        return train_data_tensor, train_label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdfbc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_dataloader(train_data_tensor, train_label_tensor, val_data_tensor, val_label_tensor, batch_size):\n",
    "        train_dataset = TensorDataset(train_data_tensor, train_label_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "        val_dataset = TensorDataset(val_data_tensor, val_label_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b857b7",
   "metadata": {},
   "source": [
    "### Function to debug LSTM classifier\n",
    "\n",
    "Function to test whether the LSTM model can theoretically learn. Take a small batch of data, \\\n",
    "and try to overfit the data. If the error can approach zero, that means that the classifier can learn.\\\n",
    "Sanity check to make sure that the model works, and that we just need to fine tune certain\\\n",
    "parameters, rather than the model actually not functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b9aeacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():# grab a tiny batch\n",
    "        # Convert to PyTorch tensors\n",
    "        train_data_tensor, train_label_tensor = create_tensors(train_set_tokenized, train_labels)\n",
    "        val_data_tensor, val_label_tensor = create_tensors(val_set_tokenized, val_labels)\n",
    "\n",
    "        # Create dataset and dataloader, set batch size\n",
    "        train_loader, val_loader = create_dataloader(train_data_tensor, train_label_tensor,val_data_tensor, val_label_tensor, 64)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        inputs, targets = next(iter(train_loader))\n",
    "        inputs, targets = inputs[:16].to(device), targets[:16].to(device)\n",
    "\n",
    "        model = LSTMClassifier(embedding_dim=128,hidden_dim = 128,n_layers=1,vocab_size=5000)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #Adam algorithm\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss()  # Binary cross entropy\n",
    "\n",
    "        # Print initial probs so we can see change\n",
    "        with torch.no_grad():\n",
    "                init_logits = model(inputs).squeeze()\n",
    "                print(\"Initial preds (sigmoid):\", torch.sigmoid(init_logits).cpu().numpy()[:5])\n",
    "\n",
    "        # try to drive loss → 0\n",
    "        model.train()\n",
    "        for epoch in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits.squeeze(), targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if epoch % 10 == 0:\n",
    "                        print(f\"[tiny overfit] epoch {epoch:3d} loss = {loss.item():.4f}\")\n",
    "\n",
    "        # Print final probs\n",
    "        with torch.no_grad():\n",
    "                final_logits = model(inputs).squeeze()\n",
    "                print(\"Final preds (sigmoid):\", torch.sigmoid(final_logits).cpu().numpy()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61152337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial preds (sigmoid): [0.50147635 0.49961942 0.48961756 0.524693   0.50284606]\n",
      "[tiny overfit] epoch   0 loss = 0.6991\n",
      "[tiny overfit] epoch  10 loss = 0.3470\n",
      "[tiny overfit] epoch  20 loss = 0.0919\n",
      "[tiny overfit] epoch  30 loss = 0.0164\n",
      "[tiny overfit] epoch  40 loss = 0.0049\n",
      "[tiny overfit] epoch  50 loss = 0.0068\n",
      "[tiny overfit] epoch  60 loss = 0.0066\n",
      "[tiny overfit] epoch  70 loss = 0.0058\n",
      "[tiny overfit] epoch  80 loss = 0.0047\n",
      "[tiny overfit] epoch  90 loss = 0.0038\n",
      "Final preds (sigmoid): [0.00276094 0.00174304 0.00292544 0.00376894 0.00281168]\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c7232",
   "metadata": {},
   "source": [
    "### Find optimal learning rate of LSTM:\n",
    "Here we run some tests to find what learning rate produces the best results: \n",
    "\n",
    "|Learning Rate|Validation Accuracy|\n",
    "|---|---|\n",
    "|0.0001|76.67%|\n",
    "|0.001|82.63%|\n",
    "|0.01|86.10%|\n",
    "|0.1|57.43%|\n",
    "\n",
    "According to these results, the accuracy of the model increases as the learning rate increases, \\\n",
    "starting at ~77% at LR = 0.0001 to its peak at ~86% at LR = 0.01. As the learning rate gets even \\\n",
    "larger, the accuracy takes a big dip, with ~57% accuracy at LR = 0.1, which is only slightly \\\n",
    "better than guessing. \n",
    "\n",
    "The 0.0001 LR is okay, but it is not the best because the learning rate might be too low, which \\\n",
    "results in the model likely underfitting on the data. This is likely true due to the fact that \\\n",
    "the gradual increase in the learning rate slowly increases the accuracy, resulting in a more \\\n",
    "tight fit on the data. On the other hand, a LR of 0.1 is far to high and results in the model \\\n",
    "overshooting and not converging on a solution.\n",
    "\n",
    "It also seems like for LR = 0.001, the BCELoss came to a stop at a minimum of 0.35, whereas \\\n",
    "for LR = 0.01 the BCELoss continued to show a downward trend at the second epoch, reaching a \\\n",
    "value of around 0.30. Which seems promising as the BCELoss might still decrease as the epochs \\\n",
    "increase.\n",
    "\n",
    "**Therefore we will use the optimal learning rate, 0.01, for the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39d03e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def learning_rate_experiment(lr): \n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        train_data_tensor, train_label_tensor = create_tensors(train_set_tokenized, train_labels)\n",
    "        val_data_tensor, val_label_tensor = create_tensors(val_set_tokenized, val_labels)\n",
    "\n",
    "        # Create dataset and dataloader, set batch size\n",
    "        train_loader, val_loader = create_dataloader(train_data_tensor, train_label_tensor,val_data_tensor, val_label_tensor, 64)\n",
    "\n",
    "        # We set the training loss here\n",
    "        loss_fn = nn.BCEWithLogitsLoss()  # Binary cross entropy\n",
    "        \n",
    "        #set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "        for learning_rate in lr:\n",
    "                epochs = 2 #keep epoch count low to test\n",
    "\n",
    "                model = LSTMClassifier(embedding_dim=128,hidden_dim = 128,n_layers=1,vocab_size=5000)\n",
    "                model.to(device)\n",
    "                \n",
    "                #set optimizer\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #Adam algorithm\n",
    "\n",
    "                print(\"learning rate {}:\".format(learning_rate))\n",
    "                train_model(epochs, model, train_loader, device, optimizer, loss_fn, val_loader)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fad6057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.0001:\n",
      "Epoch: 1, BCELoss: 0.6878650635480881\n",
      "Epoch: 1, BCELoss: 0.6800026500225067\n",
      "Epoch: 1, BCELoss: 0.6723925423622131\n",
      "Epoch 1/2 — Avg Train Loss: 0.6774 | Val Loss:   0.6572 | Val Acc:    0.6247\n",
      "Epoch: 2, BCELoss: 0.6194528722763062\n",
      "Epoch: 2, BCELoss: 0.5557245442271233\n",
      "Epoch: 2, BCELoss: 0.5087797981500626\n",
      "Epoch 2/2 — Avg Train Loss: 0.5525 | Val Loss:   0.4897 | Val Acc:    0.7667\n",
      "\n",
      "learning rate 0.001:\n",
      "Epoch: 1, BCELoss: 0.6366023629903793\n",
      "Epoch: 1, BCELoss: 0.5180443289875984\n",
      "Epoch: 1, BCELoss: 0.4651417461037636\n",
      "Epoch 1/2 — Avg Train Loss: 0.5260 | Val Loss:   0.3880 | Val Acc:    0.8293\n",
      "Epoch: 2, BCELoss: 0.36244247272610663\n",
      "Epoch: 2, BCELoss: 0.3836392605304718\n",
      "Epoch: 2, BCELoss: 0.3698554764688015\n",
      "Epoch 2/2 — Avg Train Loss: 0.3709 | Val Loss:   0.4033 | Val Acc:    0.8263\n",
      "\n",
      "learning rate 0.01:\n",
      "Epoch: 1, BCELoss: 0.6356582817435265\n",
      "Epoch: 1, BCELoss: 0.4761842629313469\n",
      "Epoch: 1, BCELoss: 0.4456121869385242\n",
      "Epoch 1/2 — Avg Train Loss: 0.5039 | Val Loss:   0.3983 | Val Acc:    0.8303\n",
      "Epoch: 2, BCELoss: 0.34809206873178483\n",
      "Epoch: 2, BCELoss: 0.31467285484075547\n",
      "Epoch: 2, BCELoss: 0.30987001046538354\n",
      "Epoch 2/2 — Avg Train Loss: 0.3206 | Val Loss:   0.3269 | Val Acc:    0.8610\n",
      "\n",
      "learning rate 0.1:\n",
      "Epoch: 1, BCELoss: 0.7289242094755173\n",
      "Epoch: 1, BCELoss: 0.7110444509983063\n",
      "Epoch: 1, BCELoss: 0.727776974439621\n",
      "Epoch 1/2 — Avg Train Loss: 0.7270 | Val Loss:   0.7158 | Val Acc:    0.5587\n",
      "Epoch: 2, BCELoss: 0.7193886375427246\n",
      "Epoch: 2, BCELoss: 0.7282245284318924\n",
      "Epoch: 2, BCELoss: 0.7272708505392075\n",
      "Epoch 2/2 — Avg Train Loss: 0.7252 | Val Loss:   0.7324 | Val Acc:    0.5743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate_experiment([0.0001, 0.001, 0.01, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c25d8a",
   "metadata": {},
   "source": [
    "### Find Optimal Batch Size of LSTM:\n",
    "\n",
    "we will try different batch sizes of the LSTM:\n",
    "32, 64, 128, 256\n",
    "\n",
    "currently: LR = 0.01\n",
    "\n",
    "|Batch Size|Validation Accuracy|Min Val Loss|\n",
    "|---|---|---|\n",
    "|32|86.43%|0.33|\n",
    "|64|86.00%|0.32|\n",
    "|128|86.60%|0.31|\n",
    "|256|85.50%|0.35|\n",
    "\n",
    "It seems like batch sizes 32-128 seem to perform similarly, with 256 slightly underperforming \\\n",
    "compared to the other batch sizes. However, out of all the different batch sizes, 128 seems to \\\n",
    "have slightly better perfomance compared to all the others, with the highest validation accuracy \\\n",
    "(86.60%) and the lowest minimum validation loss (0.31).\n",
    "\n",
    "**The batch size 128 will be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f12e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def batch_size_experiment(bs): \n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        train_data_tensor, train_label_tensor = create_tensors(train_set_tokenized, train_labels)\n",
    "        val_data_tensor, val_label_tensor = create_tensors(val_set_tokenized, val_labels)\n",
    "\n",
    "        # We set the training loss here\n",
    "        loss_fn = nn.BCEWithLogitsLoss()  # Binary cross entropy\n",
    "        \n",
    "        #set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        epochs = 2 #keep epoch count low to test\n",
    "\n",
    "        for batchsize in bs:\n",
    "                # Create dataset and dataloader, set batch size\n",
    "                train_loader, val_loader = create_dataloader(train_data_tensor, train_label_tensor,val_data_tensor, val_label_tensor, batchsize)\n",
    "\n",
    "                model = LSTMClassifier(embedding_dim=128,hidden_dim = 128,n_layers=1,vocab_size=5000)\n",
    "                model.to(device)\n",
    "\n",
    "                #set optimizer\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.01) #Adam algorithm\n",
    "\n",
    "                print(\"batch size {}:\".format(batchsize))\n",
    "                train_model(epochs, model, train_loader, device, optimizer, loss_fn, val_loader)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1f99063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 32:\n",
      "Epoch: 1, BCELoss: 0.6362595546245575\n",
      "Epoch: 1, BCELoss: 0.49490546345710756\n",
      "Epoch: 1, BCELoss: 0.43934594944119454\n",
      "Epoch: 1, BCELoss: 0.397197747528553\n",
      "Epoch: 1, BCELoss: 0.3708895942568779\n",
      "Epoch: 1, BCELoss: 0.3633361154794693\n",
      "Epoch 1/2 — Avg Train Loss: 0.4409 | Val Loss:   0.3608 | Val Acc:    0.8450\n",
      "Epoch: 2, BCELoss: 0.311865691319108\n",
      "Epoch: 2, BCELoss: 0.33126242712140086\n",
      "Epoch: 2, BCELoss: 0.3560057195276022\n",
      "Epoch: 2, BCELoss: 0.33107839673757555\n",
      "Epoch: 2, BCELoss: 0.34370814450085163\n",
      "Epoch: 2, BCELoss: 0.33533686235547067\n",
      "Epoch 2/2 — Avg Train Loss: 0.3325 | Val Loss:   0.3308 | Val Acc:    0.8643\n",
      "\n",
      "batch size 64:\n",
      "Epoch: 1, BCELoss: 0.6083816361427307\n",
      "Epoch: 1, BCELoss: 0.4654716789722443\n",
      "Epoch: 1, BCELoss: 0.4003325566649437\n",
      "Epoch 1/2 — Avg Train Loss: 0.4760 | Val Loss:   0.3533 | Val Acc:    0.8497\n",
      "Epoch: 2, BCELoss: 0.29645327508449554\n",
      "Epoch: 2, BCELoss: 0.27452757745981216\n",
      "Epoch: 2, BCELoss: 0.33267221361398697\n",
      "Epoch 2/2 — Avg Train Loss: 0.3041 | Val Loss:   0.3299 | Val Acc:    0.8600\n",
      "\n",
      "batch size 128:\n",
      "Epoch: 1, BCELoss: 0.5354123073816299\n",
      "Epoch 1/2 — Avg Train Loss: 0.4979 | Val Loss:   0.3379 | Val Acc:    0.8583\n",
      "Epoch: 2, BCELoss: 0.32799780026078224\n",
      "Epoch 2/2 — Avg Train Loss: 0.3149 | Val Loss:   0.3146 | Val Acc:    0.8660\n",
      "\n",
      "batch size 256:\n",
      "Epoch 1/2 — Avg Train Loss: 0.5313 | Val Loss:   0.4731 | Val Acc:    0.7763\n",
      "Epoch 2/2 — Avg Train Loss: 0.3411 | Val Loss:   0.3564 | Val Acc:    0.8550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size_experiment([32, 64, 128, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3debc16d",
   "metadata": {},
   "source": [
    "### Find optimal hidden size of LSTM:\n",
    "\n",
    "we will try four different hidden sizes of LSTM:\\\n",
    "64, 128, 256, 512\\\n",
    "`embedding_dim` will be the same as `hidden_dim` size. \\\n",
    "\n",
    "currently: LR = 0.01, Batch Size = 128\n",
    "\n",
    "Results: \n",
    "|Hidden Size|Binary Cross Entropy|\n",
    "|---|---|\n",
    "|64|0.692|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08923614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def hidden_dim_experiment(hidden_dimnesions): \n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        train_data_tensor, train_label_tensor = create_tensors(train_set_tokenized, train_labels)\n",
    "        val_data_tensor, val_label_tensor = create_tensors(val_set_tokenized, val_labels)\n",
    "\n",
    "        # Create dataset and dataloader, set batch size\n",
    "        train_loader, val_loader = create_dataloader(train_data_tensor, train_label_tensor,val_data_tensor, val_label_tensor, 128) #128 batch size\n",
    "\n",
    "        # We set the training loss here\n",
    "        loss_fn = nn.BCEWithLogitsLoss()  # Binary cross entropy\n",
    "        \n",
    "        for hd in hidden_dimnesions:\n",
    "                epochs = 2 #keep epoch count low to test\n",
    "\n",
    "                model = LSTMClassifier(embedding_dim=hd,hidden_dim = hd,n_layers=1,vocab_size=5000)\n",
    "\n",
    "                #set optimizer\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.01) #Adam algorithm, LR = 0.01\n",
    "\n",
    "                #select device\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model.to(device)\n",
    "\n",
    "                print(\"hidden size {}:\".format(hd))\n",
    "                train_model(epochs, model, train_loader, device, optimizer, loss_fn, val_loader)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f220d3d",
   "metadata": {},
   "source": [
    "### Find optimal number of layers of LSTM\n",
    "\n",
    "here we will try and find the optimal number of layers:\n",
    "\n",
    "|Layers|Accuracy|Min Val Loss|\n",
    "|---|---|---|\n",
    "||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc12c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden size 64:\n",
      "Epoch: 1, BCELoss: 0.5904939913749695\n",
      "Epoch 1/2 — Avg Train Loss: 0.5388 | Val Loss:   0.3982 | Val Acc:    0.8363\n",
      "Epoch: 2, BCELoss: 0.3303091543912888\n",
      "Epoch 2/2 — Avg Train Loss: 0.3378 | Val Loss:   0.3369 | Val Acc:    0.8717\n",
      "\n",
      "hidden size 128:\n",
      "Epoch: 1, BCELoss: 0.5749831840395927\n",
      "Epoch 1/2 — Avg Train Loss: 0.5198 | Val Loss:   0.3983 | Val Acc:    0.8363\n",
      "Epoch: 2, BCELoss: 0.3116022069752216\n",
      "Epoch 2/2 — Avg Train Loss: 0.3110 | Val Loss:   0.3159 | Val Acc:    0.8683\n",
      "\n",
      "hidden size 256:\n",
      "Epoch: 1, BCELoss: 0.544188356101513\n",
      "Epoch 1/2 — Avg Train Loss: 0.4748 | Val Loss:   0.3339 | Val Acc:    0.8653\n",
      "Epoch: 2, BCELoss: 0.27497520998120306\n",
      "Epoch 2/2 — Avg Train Loss: 0.2825 | Val Loss:   0.3319 | Val Acc:    0.8587\n",
      "\n",
      "hidden size 512:\n",
      "Epoch: 1, BCELoss: 0.5169095033407212\n",
      "Epoch 1/2 — Avg Train Loss: 0.4585 | Val Loss:   0.3495 | Val Acc:    0.8563\n",
      "Epoch: 2, BCELoss: 0.25979474648833273\n"
     ]
    }
   ],
   "source": [
    "hidden_dim_experiment([64, 128, 256, 512])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
